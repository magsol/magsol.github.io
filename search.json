[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Shannon Quinn. I’m an associate professor in the School of Computer Science and Department of Cellular Biology at the University of Georgia. Which is ironic at some level, considering my undergraduate degree in Computer Science came from Georgia Tech (GO JACKETS).\nMy interests are pretty broad, but the best summary I can give is that I enjoy hacking at large-scale data science problems. Recently I’ve been getting into DevOps, and ways of operationalizing machine learning; reproducibility is very important to me, and anything that helps make the hidden work of preprocessing, pretraining, and even precoding explicit is a good idea in my book. Perhaps unsurprisingly, I’m a huge proponent of open source development, and a big believer in open science. I try to make tools, data, and course materials all available with open source licenses, and try to give back to the open source communities whenever and however I can.\nIn my copious (lulz) free time, I enjoy board gaming, video gaming, and anything that involves working up a sweat: baseball, football, basketball, racquetball, cycling, hiking, and especially running."
  },
  {
    "objectID": "posts/20160202-nsf-crii-reviewer-feedback/index.html",
    "href": "posts/20160202-nsf-crii-reviewer-feedback/index.html",
    "title": "Reviewing a reviewed grant’s reviews",
    "section": "",
    "text": "Rejection in any context sucks. As a new faculty trying to prove himself (to himself, but most importantly, to his tenure committee), I particularly hate rejections in the context of grant proposals and paper submissions. The “nice” thing about the latter rejection is that it’s marginally easier to resubmit a manuscript elsewhere; there are more journals and conferences than ever, especially in computer science.\nLast September, I submitted a proposal in line with NSF’s 15-569 program solicitation: the Research Initiation Initiative in Computer and Information Science and Engineering.\nInsert jokes involving “initiation initiative” imagery here (sorry, I don’t have an avalanche of Advil).\nSomewhat unsurprisingly but nonetheless disappointingly, I received notification today that the proposal was rejected. I say unsurprising because the proposal itself was incredibly rushed; we hit “submit” about two minutes before deadline. It was my first major grant proposal, so there were bound to be some mishaps.\nSo: let’s get into the reviewer feedback!"
  },
  {
    "objectID": "posts/20160202-nsf-crii-reviewer-feedback/index.html#conclusions",
    "href": "posts/20160202-nsf-crii-reviewer-feedback/index.html#conclusions",
    "title": "Reviewing a reviewed grant’s reviews",
    "section": "Conclusions",
    "text": "Conclusions\nFor the vast majority of weaknesses, their immediate impetus can be summed up thusly: I started writing the grant the day before deadline. That’s a no-no, and contributed directly to the panicked flurry at the end and lack of detail in the proposal itself. Starting earlier will solve these problems.\nThe last point that genuinely concerns me, though, is the first reviewer’s comment on differentiating myself from my graduate advisors. It’s true that this proposal took work I started as a graduate student, and I agree that it’s important to build an independent research group such that I don’t have to rely on my graduate advisors for funding.\nBut I suppose I could boil this down to time constraints as well: given more elbow room, I could have provided a more detailed development plan that clearly differentiated itself from work done before. Rather than relying on the “we propose to extend…” mantra, I could have delved into much greater detail on the methods I’d wanted to use and why their application would be novel.\nSo there you have it: a review of grant reviews. Nothing left to do here but incorporate the feedback and push out more effective proposals in the months to come!"
  },
  {
    "objectID": "posts/20170531-summer-of-data-science/index.html",
    "href": "posts/20170531-summer-of-data-science/index.html",
    "title": "Summer of Data Science 2017",
    "section": "",
    "text": "A few days ago, while I was still in Portland, OR enjoying the last few days of PyCon 2017, Renee (@BecomingDataSci) mentioned on Twitter that she might bring back the “Summer of Data Science” she kicked off last year.\nIn short: use the summer as an opportunity to learn and/or accomplish something data science-related in a goal-oriented, data-driven way.\nAs a tenure-track professor still yet to endure the 3rd-year review, pretty much everything data science related revolves around furthering my eventual tenure dossier. But the summers are still considerably more flexible for me to accomplish some of these goals than the fall or spring (I love teaching, but it’s all-consuming). I also have more than just myself–networks of collaborators and research assistants who can help me with these goals.\nSo without further adieu, I’d like to put forth my own Summer of Data Science, 2017 Edition:\n\nTeaching: 1360E improvements and 4360/6360 preparation\nI’m teaching an online course this summer, starting June 5 and continuing through the end of July: CSCI 1360E Foundations of Informatics and Analytics.\nOr, more colloquially, I just call it “introduction to data science.”\nIt’s one of the five courses I proposed in my first year (a story for another time, believe me…) and taught for the first time last summer, and in-person last fall. I’m teaching it again this summer and hoping to make some of the standard improvements, such as sprucing up slides and fixing autograder bugs from previous incarnations.\nMore pressing is CSCI 4360/6360 Data Science II, which has yet to have its own website. This course has never been taught, but is slated for Fall 2017.\nDid I mention there are already over 40 students registered, undergraduate and graduate combined?\nThis is supposed to be the “corner cases” of data science: lesser-used and/or more powerful analysis methods that aren’t necessarily the first tools or techniques you reach for in a given situation. This will include, but certainly not be limited to, concepts like randomized algorithms, semi-supervised learning, and backpropagation; techniques like out-of-core processing, distributed computing, and functional programming; tools like Julia, numba, and dask.\nBefore you ask: No. I’m not covering Spark.\nAt any rate, the conceptual framework is in place, but practically none of the architectural logistics have been established. So this will take up virtually all of my time for the remainder of the summer, once the following item is completed…\n\n\nResearch: NSF CAREER proposal\nThe advantage of this goal is it has a hard end date: July 19.\nThe disadvantage is that the NSF CAREER is the pinnacle of NSF early-career investigator awards. Its importance cannot be overstated; its je ne sais quoi is on display in the faces of new investigators when you mention it around them.\nI’ve connected with my university’s grant proposal officers, and am participating in a peer review process that involves two rounds of review: first with fellow CAREER submitees (as in, we exchange each other’s current drafts!), and second with the university grant officers.\nAn investigator can only make three submission attempts for the CAREER. The CAREER is only open for submissions once per year. If chosen, the investigator must still be un-tenured at time of award. These conditions place brutal limits on how long you can wait to start making submissions, and I already skipped submitting my first two years. Still, I have no illusions about the uphill battle I have, so my goal this first time around is simply to acquire feedback: from my peers, from my grants officers, and from the NSF reviewers.\n\n\nPersonal: Coursera’s optimization course\nI signed up to take Coursera’s Discrete Optimization course out of the University of Melbourne, taught by none other than Professor Pascal Van Hentenryck at Michigan and Dr. Carleton Coffrin, currently at Los Alamos.\nOptimization is pretty much the backbone of modern machine learning, and yet I have almost no formal training in it–beyond incidentally covering it in various machine learning and statistics courses as part of some broader topic in which it’s used. Oddly, the most rigorous instruction I received as a graduate student in optimization was a Cell & Systems Modeling course where we used various optimization techniques to find parameter values for population models.\nEspecially in conjunction with what I hope to teach in CSCI 4360 in the fall, this would be invaluable both professionally as well as personally."
  },
  {
    "objectID": "posts/20151107-introductions/index.html",
    "href": "posts/20151107-introductions/index.html",
    "title": "Introductions",
    "section": "",
    "text": "Well, this seems to be working. Kinda.\nI took some inspiration from Jake VanderPlas’ Pythonic Perambulations and opted for a similar route: Pelican as the backend blogging machine, github as the host, and (eventually) embedded Jupyter notebooks. Unfortunately, Jake’s addition to the Pelican plugins to allow Jupyter notebooks uses CSS that is, shall we say, unfriendly to Pelican themes outside of the Octopress default.\nSo that part is still incomplete. As are many of the links and some of the plugins. But ultimately, I’m hoping to use this to supplant my current pair of blogs that jointly detail what my plan for this space is: academia, general data science, and other musings.\nStay tuned!"
  },
  {
    "objectID": "posts/20160516-autopost-running-twitter-summary-strava/index.html",
    "href": "posts/20160516-autopost-running-twitter-summary-strava/index.html",
    "title": "A twitterbot for posting weekly running stats",
    "section": "",
    "text": "We runners (we’re a crazy bunch), for the most part, like our stats. How many miles do you log each week? Each month? How are your average race paces trending? Are your long runs both feeling good and getting faster?\nYes, we’re a little obsessed with our numbers.\nIt’s no surprise, then, that web services have popped up to help us aggregate some of these numbers. One of the most obvious is Garmin Connect, home for pretty much anyone who uses the Garmin GPS watches.\nAnother that I’ve used before is Daily Mile. However in recent months I’ve become frustrated enough with the site to leave entirely. By all accounts, no part of the site has been updated in years, and other alternatives are simply much more pleasant to use.\nUnfortunately, there was one crucial feature of Daily Mile I really liked: connecting it to your Twitter account to post weekly summaries of your recorded workouts. I liked it so much, in fact, I created a small web service to do the same thing, but for monthly summaries:\n\nObviously that service no longer exists, but ever since I’ve been wanting to get something similar up and running again. Especially now that I don’t even have weekly summaries anymore–just couldn’t stomach Daily Mile any longer–I wanted to take the opportunity (and the shiny new blog) to go through a step-by-step procedure of creating your own Twitter / Strava app for posting weekly summaries on Twitter of your running mileage!\n\nPreliminaries\nA few things you’ll need before we get started:\n\nPython 3.5\ntweepy (for interfacing with Twitter)\nstravalib (for interfacing with Strava)\npybot (shameless plug, but it will help)\n\nBoth Twitter and Strava use OAuth as their method of app authentication. These libraries just make it easier to interact with the services; they abstract a lot of the details of authentication and communication.\nBut in case you’re interested: Twitter’s docs and Strava’s docs.\n\nStep 1: Create a Strava app\nGo to your user settings and create an app that can interface with your account. Important pieces of information you’ll need later: Client ID, Client Secret, and Your access token.\nWe can test if it works. Fire up an IPython terminal, get your access_token, and run the following code:\n    from stravalib import Client\n    client = Client(access_token = access_token)\n    client.get_athlete()\nYou should see something along the lines of:\n    Out[]: <Athlete id=1234567 firstname=b'Firstname' lastname=b'Lastname'>\n\n\nStep 2: Retrieve last week’s data\nThe whole point is to get weekly mileage reports. Thankfully, the get_activities method in stravalib has an after optional parameter we can use to precisely tune what time interval we want.\nFirst, though, we need to create a timestamp that represents the 1-week time frame. If we assume this will only be executed on the day we want the summary–say, every Monday–then we need to tally up all the runs from the day before all the way back to the previous Monday, inclusive.\n    import datetime\n    current = datetime.datetime.now()\n    last_week = current + datetime.timedelta(weeks = -1)\n    after = datetime.datetime(last_week.year, last_week.month, last_week.day)\n    activities = client.get_activities(after = after)\nAssuming we run this chunk of code on a Monday, it should give us every Strava activity from the previous Monday up to the present.\nHowever, we’re not done yet. This includes everything–not just our runs, but any other activities that we recorded; yoga, weights, elliptical, and so on. We need to filter these out. We also need to filter out the edge case of any activities that have been recorded today, since we don’t want to include these in a report of last week’s activities!\n    l = [activity.id for activity in activities if activity.type == 'Run' \\\n        and a.start_date_local.day != current.day]\nOk, let’s pause and discuss what’s happening here.\nFirst, the most obvious: we’re looping through the activities generator we obtained in the last line of the previous step. Second, the if statement at the end filters out any activities that aren’t a run. Finally, the activity.id part out front says, we’re building a list of the unique IDs that identify each activity. The last part is our timeframe edge case: if we recorded an activity today, even a running activity, don’t include it.\nWhy are we only holding onto the IDs? It has to do with detail. Strava maintains a hierarchy of details available to users that vary with authentication, connection, etc. Simply put, when we request a list of activities, the default detail level is 2, which is “summary” level. However, some of the metrics we need–calories in particular!–require level 3, or “detailed”. To get this level of detail, we need to query for individual activities…one at a time.\nHence, a list of activity IDs! Now we can loop through the IDs, requesting details on each run and tabulating up the mileage and calories.\n    from stravalib import unithelper\n\n    mileage = 0.0, calories = 0.0\n    for activity_id in l:\n        activity = client.get_activity(activity_id)\n\n        # This is annoying; all the default distances are in meters! Luckily,\n        # stravalib comes with a nice unit helper utility to do the conversion.\n        distance = unithelper.miles(activity.distance)\n        mileage += round(distance.num, 2)  # Rounds to 2 sig figs.\n        calories += activity.calories\nThere you have it! In those two variables–mileage and calories–you have all the data you need to summarize your running workouts for the last week. Now we just need to post this information on Twitter!\n\n\nStep 3: Create a PyBot\nOk, time for a shameless plug: yes, I’m the pybot author. It’s still highly experimental, and largely uncompleted, but for our purposes it will suffice nicely as a barebones framework to interact with Twitter.\nClone the repo and follow the setup script to create a Twitter app and connect it to your account.\n    git clone https://github.com/magsol/pybot.git\n    cd pybot\n    sbin/create_pybot.py\nThat will walk you through the instructions for creating an app, generating OAuth credentials, and stubbing out your first pybot. Feel free to give it whatever name you’d like; for the purposes of this tutorial, I’ll assume we’ve named it artbot (don’t ask). Congratulations! You’ve created a twitter bot!\n\n\nStep 4: Customize the bot’s behavior\nOur bot is pretty simple: every Monday at some specified time, it will wake up, read all the prior week’s running activities, and post the summary before going back to sleep for another week.\nIt won’t be prompted by anything other than time. So the specific action override we’re looking for in PyBot parlance is on_tweet, and the interval we’ll use is tweet_interval. The latter is easy enough–a full week between tweets!\n    self.config['tweet_interval'] = 60 * 60 * 24 * 7\nBefore we go any further: does anyone see anything wrong with the above code snippet?\nI’ll give you a hint: imagine you started this bot on a Tuesday, instead of a Monday.\nYep, there it is. This interval we’ve defined is exactly 1 week, but it doesn’t account for when we actually START the bot. We need this to be a little more intelligent. If you want the posting to happen weekly every Monday, it shouldn’t matter when you actually start running the bot, right? It should be smart enough to figure out when it needs to post for the first time, then post weekly thereafter.\nA neat component of PyBot is that, in addition to giving hard time frames, you can also specify functions to compute the interval on-the-fly, subject to some other constraints that are dynamic (like on what day of the week you happen to fire up the bot).\nTo make things easy on us, we’ll use the datetime convention in the Python documentation for identifying individual days of the week. This tutorial assumes Mondays (which corresponds to 0), but you can use whatever value you want.\nWe need to store this as a configuration parameter in the bot.\n    # Put this somewhere in the bot_init() method\n    self.config['update_day'] = 0  # Corresponds to Monday.\nNow, we need a function to compute the interval between updates.\n    # Put this somewhere in the bot_init() method\n    self.config['tweet_interval'] = self._compute_interval\nWe’ve referenced an internal method we’re calling _compute_interval, as of yet undefined. Let’s go ahead and define it!\n    # Put this somewhere in the bot class declaration\n    def _compute_interval(self):\n        interval = 60 * 60 * 24 * 7  # The default interval; we'll start here\n\n        # Are we on the right day of the week?\n        now = datetime.datetime.now().weekday()\n        target = self.config['update_day']\n        if now == target:\n            return interval  # Nothing to do! Yay!\n\n        # If we get to this point, it means the index of the current day--\n        # as in, right when the code gets HERE--doesn't match the index of the\n        # day we've said we want to perform this update. So we need to do a\n        # little bit of work to compute that date.\n\n        if now > target:\n            # This is a hack, so the index of the CURRENT day will always be\n            # smaller than the index of the TARGET day.\n            now -= 7\n\n        # Essentially, all we've done is replace the 7 above with whatever\n        # it needs to be in order to get us to our target day.\n        return (target - now) * 24 * 60 * 60\nNow that our interval is in place, we’ll need to override the on_tweet action to do what we want whenever it’s called (which will be once each week on the day we’ve specified!). Remember, this method is called once we’ve hit our interval. So this is where it all comes together!\n    def on_tweet(self):\n\n        # First, pull in the stats from Strava.\n        current = datetime.datetime.now()\n        last_week = current + datetime.timedelta(weeks = -1)\n        after = datetime.datetime(last_week.year, last_week.month, last_week.day)\n        activities = client.get_activities(after = after)\n\n        # Second, filter by activity type and time frame.\n        l = [activity.id for activity in activities if activity.type == 'Run' and\n            a.start_date_local.day != current.day]\n\n        # Third, tabulate up the stats for mileage and calories.\n        mileage = 0.0, calories = 0.0\n        for activity_id in l:\n            activity = client.get_activity(activity_id)\n            distance = unithelper.miles(activity.distance)\n            mileage += round(distance.num, 2)  # Rounds to 2 sig figs.\n            calories += activity.calories\n\n        # Finally, use the stats to craft a tweet. This can be any format\n        # you want, but I'll use the example one from the start of the post.\n        tweet = \"My training last week: {:d} workouts for {:.2f} miles and {:d} calories burned.\".format(len(l), mileage, calories)\n        self.update_status(tweet)\nThat’s it! You have everything you need; now, just set the bot to run ad nauseum:\n    python artbot.py\nIt should run forever, sleeping for most of it but waking every week to post your summary. If you notice something isn’t working right, check the logs; they should specify if there are problems e.g. with permissions posting to Twitter, or connections hanging and disconnecting.\n\n\n\nConclusion\nThat’s all there is to it! There are obviously a lot of technical hurdles I largely glossed over–creating the apps for both Strava and Twitter can be a little more involved than the average person would like, and Python versions (especially 2.x vs 3.x) can wreak havoc on your code. I tried to be as reproducible as I could, though until Jupyter notebooks decide to play nice with Pelican (or maybe the other way around?) these code embeddings will have to suffice. Sigh.\nPlease feel free to leave a comment if you have any questions! I’ve also posted the bot in the examples folder in the pybot GitHub repository as artbot.py. Happy tweeting!"
  },
  {
    "objectID": "posts/20170316-so-you-want-to-conduct-research-with-me/index.html",
    "href": "posts/20170316-so-you-want-to-conduct-research-with-me/index.html",
    "title": "So, you want to conduct research with me?",
    "section": "",
    "text": "Deviation #1: This is wholly separate from the “should I get a Ph.D.” question. For that I would recommend one of numerous guides that ask all the right questions.\nDeviation #2: This is also wholly separate from how to succeed in a Ph.D. program, though there is some overlap. In general, there are some key items to consider and habits to build as you work your way through a Ph.D., and for those questions I highly recommend Andrej Karpathy’s excellent survival guide to a Ph.D., some of which may be mirrored here.\nNo, the specific question I’m addressing here is how to succeed in graduate research as one of my students. In order of somewhat-importance:\n\n1: Familiarize yourself with my research interests.\nThis may seem like a no-brainer, but you’d be surprised how many emails I get from individuals expressing profound interest in working with me, only to see they have i) no experience whatsoever in any of my interests (which isn’t necessarily a deal-breaker!), and ii) don’t appear to know what I work on (which is a deal-breaker). If you work with me, you’re going to do something at the intersection of bioimaging + distributed computing + biosurveillance; something that involves computer vision and machine learning in a public health setting.\nCheck out my Google Scholar record, my group’s GitHub repositories, and my lab website to get at least a basic idea of the sort of work I do, and to jog your thinking along the lines of what you might be able to contribute.\n\n\n2: Take into consideration the following expectations.\nWork hard, play hard, and be able to talk about both:\n\nI expect my students to develop into excellent scientific communicators. In practice, this means I want you to be able to talk about your work to experts in the same field, experts in different fields, and even non-scientists. I want you to get comfortable giving talks and writing papers. If English isn’t your first language, that’s ok! Just be aware this expectation may take more time than you think.\nI expect my students to participate in Open Science. In practice, this means I want you to publish all the code you write in public repositories, mirror all your papers on arXiv, participate in and contribute to open source projects, and maybe even contribute to a blog. Research is only interesting when you can pick through the code, explore the data, and regenerate the results. Know or learn how to use scientific notebooks, version control, wikis, and even containers!\nI expect my students to demonstrate a promising slope of accomplishment. Put simply, I recruit for potential (slope), not experience (y-intercept). You don’t have to know everything about machine learning, statistics, and linear algebra, and be an expert programming in Python just to be able to work with me. However, I do expect that you will be able to pick up these skills very quickly.\nI expect my students to take ownership of their projects, pushing the envelope of what is known and beyond what even I would suggest.\n\nSpeaking of which…\n\n\n3: Prepare yourself to be a self-sufficient researcher.\nThis does NOT mean I expect you to do everything! The whole point of being a student is that you have a mentor, a supervisor who is (in theory) more experienced than you, from whom you can learn. It is indeed my job to guide you and teach you what I know, and I will most certainly do that.\nThis DOES mean that, by the end of your time here, I want you to be the expert on your project! You should take the project and run with it, rather than wait for me to tell you what to do next. Build your intuition about the problem through “moving fast and breaking things,” to use the tech startup parlance. Ask forgiveness rather than permission. Insert witticism here that basically says KEEP TRYING THINGS.\nEven as you start your project, you may come across roadblocks that I don’t have an answer for. That’s how research works: we’re pushing the limits of our collective knowledge! I may have some intuition from previous problems, and I’ll certainly share that, but asking me “What do I do next?” is going to frustrate both of us very, very quickly.\nHere’s another example: the levels of data science classes. You don’t have to be anywhere above Level 2 when you arrive (maybe you’re still working your way through Level 1!), but I expect you to work through Level 7 by the time you leave.\n\n\nIf you’ve made it this far, and are still interested…\n…then I encourage you to reach out to me and express your interest! I have just a few final pieces of advice:\nFirst, read this brief Twitter thread and this brief Twitter thread as well.\nNext, do NOT send me\n\na form email with a 10-page summary of your accomplishments (as I said, I don’t hire for y-intercept, I hire for slope)\nan email with misspellings or grammatical errors (recall that I emphasize good scientific communication skills)\na message that starts with “Dear Ms Quinn” (first, I’m a doctor; second, I’m a guy)\n\nI won’t respond.\nFinally, write an email that’s 3-5 sentences at most, with a specific mention of the work you find interesting and want to pursue further. For extra credit, mention some kind of improvement or extension to the work that you came up with on your own. This tells me a lot of things all at once, most important of which is that you’re serious about wanting to work with me.\nBecause if you can handle graduate school…\n\n…I really do believe you can handle anything."
  },
  {
    "objectID": "posts/20221213-pardon-the-dust/index.html",
    "href": "posts/20221213-pardon-the-dust/index.html",
    "title": "Pardon de dust (2)",
    "section": "",
    "text": "Why hello there. You may have noticed that things look a little bit different around here.\nI’m in the process of migrating my blog off Pelican and on to Quarto.\nWhy, you ask? Honestly, I got tired of having to fix a broken plugin or failed git push every. single. time. that I finished writing out a blog post. It gave an already-infrequent task an even higher activation threshold, and during a stretch where spare bandwidth was an incredible luxury.\nIn short: I’d really like to blog more often, and from what I’ve seen, Quarto will help me do just that. It definitely requires some extra bandwidth to get things migrated over, but fortunately: I have some of that at the moment :)\nStay tuned! In the meantime, please enjoy the above recent photo of Clover helping us put up our tree."
  },
  {
    "objectID": "posts/20160926-osbd2016-workshop/index.html",
    "href": "posts/20160926-osbd2016-workshop/index.html",
    "title": "Open Science in Big Data (OSBD) Workshop",
    "section": "",
    "text": "Back in May of this year, some of the other computer science professors and I put together a proposal for a workshop. The workshop would be in conjunction with the December 2016 IEEE BigData Conference, and would focus specifically on the intersection of big data and open science.\nIn June, we got the good news: it was accepted!\n\nWhat is “Open Science”?\nI’ve been interested in open science since I started graduate school. I’d been using Apache projects since college, learning how to set up and configure my own LAMP stack (yes, I’ll admit I dabbled in PHP for a few years, but I’m clean now!), but had never really contributed to an open source project until the 2010 Google Summer of Code. During that time, I worked on adding spectral clustering the Apache Mahout library.\nWhile open source is great and wonderful, this is just one component of Open Science. It’s another reason I’m incredibly excited about the “container revolution”: Docker and its ilk. This concept makes entire pipelines perfectly reproducible, to the point where it’s now possible for entire scientific projects to be replicated with a single command: from running the analysis, to building the figures, to assembling the final PDF file (which, by the way, can be uploaded to arXiv for everyone to read!)\nThese other aspects–open sourcing data, reproducibility scripts, and entire pipelines–are veritable educational and research gold mines, but are arguably much more difficult than simply dumping some code on GitHub. These difficulties are only exacerbated in the realm of “big data”.\n\n\nWhat are the goals of the workshop?\nThe goals of the workshop are threefold.\nFirst: we aim to identify some of the biggest challenges in democratizing big data analytics. Open Science is a critical component of both educational pedagogy and scientific research. With the increasing relevance of big data analytics, making these tools and resources available to the next generation of scientists and big data practitioners is crucial. Unfortunately, this is also extremely challenging.\nSecond: we aim to bring together big data practitioners from multiple backgrounds to discuss and establish the current state of affairs with respect to reproducibility in big data analytics and machine learning. Priorities change and constraints differ between researchers and developers in academia versus industry versus government; nonetheless, open science and big data are important to practitioners in each of these areas.\nThird: we aim to examine and propose possible routes forward to advance the continued integration of open science in big data analytics, putting tools, techniques, data, and documentation in the hands of researchers, students, and other big data practitioners. We will identify emerging trends in terms of open science best practices, and how these can be incorporated into current big data endeavors.\n\n\nWhat kinds of proceedings is the workshop accepting?\nWe hope to have two tracks: a full research paper session, and a student (short) paper session.\nPapers submitted to the OSBD workshop should, at the very minimum, have a significant Open Science component. This can take several forms, including but not limited to any combination of the following:\n\nUses free and open source packages\nOpenly available online, e.g. GitHub\nScripts are available to reproduce figures\nData are openly available e.g. dat, datahub.io\netc.\n\nWe are welcoming of original research into big data analytics, so long as there is an open science component.\nFurthermore, in the spirit of Open Science and reproducibility, we strongly encourage our presenters to incorporate live code demos, data walkthroughs, or some other hands-on activity into their talks.\n\n\nWhen is the submission deadline?\nThe deadline for workshop paper submissions is Sunday, October 25, 2016. This provides a 2-week buffer zone between the notification deadline of the main IEEE BigData conference, in case you want to repackage your conference submission for the workshop.\n\n\nWhere can one find more information about OSBD?\nWe have a website: https://osbd.github.io/\nThe website contains information about the topics around the workshop, the invited speakers who will be attending (keynote and panelists), the important dates around the workshop, and the instructions for formatting and submitting your paper to the workshop.\nWe are very excited about this workshop, and encourage you to submit! We look forward to seeing you in December!\n\n\nShannon Quinn, Co-chair\nJohn Miller, Co-chair\nSuchi Bhandarkar, Co-chair\nYi Hong, Co-chair"
  },
  {
    "objectID": "posts/20170106-gameify-your-raspberry-pi/index.html",
    "href": "posts/20170106-gameify-your-raspberry-pi/index.html",
    "title": "Game-ify Your Raspberry Pi",
    "section": "",
    "text": "My project over the 2016 holiday season was to take the Raspberry Pi 3 I’ve had sitting around the house idling for the previous nine months and turn it into a RetroPie-ified gaming emulator.\nRetroPie is a phenomenal bit of software that combines the work of several projects into a single package that was primarily designed to be deployed on Raspberry Pis, but which can also be set up on a regular ol’ PC. If you’re using a Raspberry Pi, it’s built on top of the Raspbian OS, so all the commands familiar to you will still be available.\nI have to say up front: the folks with RetroPie have done an amazing job. The documentation was excellent, and while I’ll walk through the installation and configuration here, with only one notable exception everything went exactly as prescribed.\n\nRetroPie\nAssuming you have all the needed hardware, installing the RetroPie OS on your Pi is pretty straightforward.\n\nUse some sort of imaging software to write the RetroPie image to an SD card (I used ApplePi Baker without any problems, but you could also just use the dd command in the Terminal, which directly moves bytes from one place to another).\nInsert the SD card into your Pi.\nBoot! I had a few false-starts that were the result of the SD card not being fully inserted, so nothing came up.\n\nIf all goes well, you should see the wonderful Raspbian logo.\n\n\n\nConfiguring RetroPie\nThe first thing you’ll need to do when RetroPie boots is to configure whatever controller you’re going to use. I went out and purchased some standard-issue Logitech USB gamepads and they seem to be functioning very well. You can also use your keyboard if you want for this, but it pretty much has to be something that can be connected via USB directly to the Pi.\nA few pointers in this step:\n\nTo skip a particular button configuration, just press and hold any button for a second.\nIf you make a mistake in configuring a button, you have to get all the way to the bottom before you can scroll back up and fix it.\nIt took me 5-6 tries to get this completely right. Just go back and do it again if you mess something up.\n\n\n\nEmulationStation\nOnce you successfully configure a gamepad, you’ll be taken to the main landing page: EmulationStation.\n\nWhere are teh gamez0rz?!1, you might be saying. Well, since ROMs are a legal gray area, you’re kinda on your own there. BUT! We have something else we need to do first anyway: set up a network configuration.\n\n\nNetwork Configuration\nIf you have a Raspberry Pi 2 or earlier, your network choices are limited to an ethernet connection. With the advent of Raspberry Pi 3s, suddenly wifi is an option! Turns out, I have a 3, and it is this wifi that caused me a couple hours’ grief until I figured out what was going on.\nGo to the wifi configuration in EmulationStation. You’ll be taken to a network selection screen–protip, the Pi 3 cannot connect to 5GHz networks–where you can select your wifi network.\nYou’ll then need to type in the wifi password. Provided you get it right, that should be it! You’re all set to go!\nIf you find yourself entering your password again…and again…and again…and again…and can’t figure out why it’s failing, read on.\n\n\nKeyboard Layout\nFor whatever reason, my keyboard was misconfigured–the layout was set to English (U.K.), resulting in some special characters being remapped. However, since the Raspbian password interface echoes stars * back, I had no idea I was typing a completely wrong password until I fired up a command prompt.\nLuckily, once you diagnose this issue, it’s fairly easy to fix. Raspbian has a utility built-in for changing your keyboard layout. You just have to navigate in EmulationStation to the option that gives you a command prompt.\nIn my case, once I had a working network connection, I just used SSH to perform any other operations on the Pi (e.g. uploading new games). The default username/password for RetroPie is pi/raspberry, which I suggest you change either from the EmulationStation main menu, or after your first SSH login.\n\n\nMoonlight on Raspberry Pi\nYou should have a working RetroPie at this point, so that’s cool!\nHowever, as awesome as the Pis are, they’re not exactly brimming with horsepower. As such, if you try running N64 or PS2 emulators, you may find as I did that anything requiring 3D shaders gets very jittery, to the point of unplayable. RetroPie has detailed configuration options for every emulator to help you get the absolute most out of your Pi, but there are some things a tiny CPU with an embedded stock GPU just can’t do, and one of those is render 3D shaders the way they were meant to be.\nSo what? Give up? Nay!\n\nMoonlight Embedded is the open source version of NVIDIA’s GameStream as used by the NVIDIA Shield. Basically, it’s a way of streaming the output of an NVIDIA GPU to another input, like a TV.\nIn this case, we’re basically turning our Pi into a glorified Chromecast–it’ll take the output from a dedicated GPU and stream it directly to the TV it’s connected to. Yes, this means you’re not actually playing games on the Raspberry Pi–you’re playing them on whatever computer has the GPU in it (oh and it has to be an NVIDIA GPU, GTX 650 or higher)–but all Moonlight requires is Raspbian, which is what RetroPie is built on!\nI found this installation guide perfect (make sure you download the latest versions of everything!), with the lone exception that, just before downloading and installing Moonlight itself, I also needed to install one more library:\n    sudo apt-get install libenet-dev\nInclude that with the other libraries installed just before Moonlight, and everything else should just work. The rest of the instructions take you through configuring the gamepads so they correctly send their input back to the GPU (so, y’know, you don’t have to sit at your computer while staring at your TV).\n\n\nHappy Fun Times!\nYes, lots of happy fun times. But also HappyFunTimes!\nIf you’re really into the party games, installing this gaming server may be the best thing ever. It’s a brilliant setup: gamepads are HTML5 canvases that run on smartphones, essentially mimicking controllers so everyone with a smartphone can play. The games range from stupidly simple to shockingly sophisticated. I may post configuration for that later, but I figure: if you’re building the Ultimate Gaming Pi, why not include that, too?\nAnd with that, I give you the results at our house:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogging, take 14,000,605; action!",
    "section": "",
    "text": "Pardon de dust (2)\n\n\nMigrating blog backends is hard\n\n\n\n\npersonal\n\n\nprofessional\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDecember 13, 2022\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nSummer of Data Science 2017\n\n\n\n\n\n\n\nprofessional\n\n\npersonal\n\n\ndata science\n\n\npython\n\n\ncomputer vision\n\n\nimaging\n\n\nmachine learning\n\n\ndeep learning\n\n\nnvidia\n\n\ngpu\n\n\nonline courses\n\n\nteaching\n\n\neducation\n\n\ncat detector\n\n\ngenerative adversarial networks\n\n\nlatent subspace\n\n\nmanifolds\n\n\nbioimaging\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nSo, you want to conduct research with me?\n\n\n\n\n\n\n\nprofessional\n\n\nphd\n\n\ngraduate school\n\n\nresearch\n\n\nmentoring\n\n\ncomputer science\n\n\ngraduate student\n\n\n\n\n\n\n\n\n\n\n\nMarch 16, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nGame-ify Your Raspberry Pi\n\n\n\n\n\n\n\npersonal\n\n\npi\n\n\nraspberry pi\n\n\nretropie\n\n\nretro\n\n\ngaming\n\n\nvideo games\n\n\nnvidia\n\n\nshield\n\n\nsteam\n\n\n\n\n\n\n\n\n\n\n\nJanuary 6, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nOpen Science in Big Data (OSBD) Workshop\n\n\n\n\n\n\n\nprofessional\n\n\nworkshop\n\n\nbig data\n\n\nieee\n\n\nopen science\n\n\n\n\n\n\n\n\n\n\n\nSeptember 26, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nA twitterbot for posting weekly running stats\n\n\n\n\n\n\n\npersonal\n\n\nhowto\n\n\npython\n\n\nstrava\n\n\nrunning\n\n\noauth\n\n\ntweepy\n\n\ntwitter\n\n\npybot\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nReviewing a reviewed grant’s reviews\n\n\n\n\n\n\n\nprofessional\n\n\nacademia\n\n\ngrants\n\n\nreviewer feedback\n\n\nrejection\n\n\n\n\n\n\n\n\n\n\n\nFebruary 2, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nIntroductions\n\n\n\n\n\n\n\npersonal\n\n\npelican\n\n\npython\n\n\ngithub\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNovember 7, 2015\n\n\nShannon Quinn\n\n\n\n\n\n\nNo matching items"
  }
]