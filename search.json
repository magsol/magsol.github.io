[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data, donuts, and detours",
    "section": "",
    "text": "Mastodon, Part I: My home network topology\n\n\nDNS. It’s always DNS. Except when it’s not.\n\n\n\n\npersonal\n\n\nraspberry pi\n\n\nk3s\n\n\nmastodon\n\n\nnetworking\n\n\n\n\n\n\n\n\n\n\n\nJanuary 23, 2023\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nRunning your own Mastodon instance on a Raspberry Pi k3s cluster\n\n\nNow you too can unwittingly become an unpaid sysadmin!\n\n\n\n\npersonal\n\n\nraspberry pi\n\n\nk3s\n\n\nmastodon\n\n\n\n\n\n\n\n\n\n\n\nJanuary 20, 2023\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\n2022: A year in review\n\n\nLast year doesn’t deserve a rhyming couplet\n\n\n\n\npersonal\n\n\nprofessional\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJanuary 17, 2023\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nPardon de dust (2)\n\n\nMigrating blog backends is hard\n\n\n\n\npersonal\n\n\nprofessional\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDecember 13, 2022\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nSummer of Data Science 2017\n\n\n\n\n\n\n\nprofessional\n\n\npersonal\n\n\ndata science\n\n\npython\n\n\ncomputer vision\n\n\nimaging\n\n\nmachine learning\n\n\ndeep learning\n\n\nnvidia\n\n\ngpu\n\n\nteaching\n\n\neducation\n\n\nbioimaging\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nSo, you want to conduct research with me?\n\n\n\n\n\n\n\nprofessional\n\n\nphd\n\n\ngraduate school\n\n\nresearch\n\n\nmentoring\n\n\ncomputer science\n\n\ngraduate student\n\n\n\n\n\n\n\n\n\n\n\nMarch 16, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nGame-ify Your Raspberry Pi\n\n\n\n\n\n\n\npersonal\n\n\nraspberry pi\n\n\ngaming\n\n\nvideo games\n\n\nnvidia\n\n\nsteam\n\n\n\n\n\n\n\n\n\n\n\nJanuary 6, 2017\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nOpen Science in Big Data (OSBD) Workshop\n\n\n\n\n\n\n\nprofessional\n\n\nworkshop\n\n\nbig data\n\n\nieee\n\n\nopen science\n\n\n\n\n\n\n\n\n\n\n\nSeptember 26, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nA twitterbot for posting weekly running stats\n\n\n\n\n\n\n\npersonal\n\n\npython\n\n\nstrava\n\n\nrunning\n\n\noauth\n\n\ntweepy\n\n\ntwitter\n\n\npybot\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nReviewing a reviewed grant’s reviews\n\n\n\n\n\n\n\nprofessional\n\n\nacademia\n\n\ngrants\n\n\nfeedback\n\n\nrejection\n\n\n\n\n\n\n\n\n\n\n\nFebruary 2, 2016\n\n\nShannon Quinn\n\n\n\n\n\n\n  \n\n\n\n\nIntroductions\n\n\n\n\n\n\n\npersonal\n\n\npelican\n\n\npython\n\n\ngithub\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNovember 7, 2015\n\n\nShannon Quinn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-09-26-osbd2016-workshop/index.html",
    "href": "posts/2016-09-26-osbd2016-workshop/index.html",
    "title": "Open Science in Big Data (OSBD) Workshop",
    "section": "",
    "text": "Back in May of this year, some of the other computer science professors and I put together a proposal for a workshop. The workshop would be in conjunction with the December 2016 IEEE BigData Conference, and would focus specifically on the intersection of big data and open science.\nIn June, we got the good news: it was accepted!\n\nWhat is “Open Science”?\nI’ve been interested in open science since I started graduate school. I’d been using Apache projects since college, learning how to set up and configure my own LAMP stack (yes, I’ll admit I dabbled in PHP for a few years, but I’m clean now!), but had never really contributed to an open source project until the 2010 Google Summer of Code. During that time, I worked on adding spectral clustering the Apache Mahout library.\nWhile open source is great and wonderful, this is just one component of Open Science. It’s another reason I’m incredibly excited about the “container revolution”: Docker and its ilk. This concept makes entire pipelines perfectly reproducible, to the point where it’s now possible for entire scientific projects to be replicated with a single command: from running the analysis, to building the figures, to assembling the final PDF file (which, by the way, can be uploaded to arXiv for everyone to read!)\nThese other aspects–open sourcing data, reproducibility scripts, and entire pipelines–are veritable educational and research gold mines, but are arguably much more difficult than simply dumping some code on GitHub. These difficulties are only exacerbated in the realm of “big data”.\n\n\nWhat are the goals of the workshop?\nThe goals of the workshop are threefold.\nFirst: we aim to identify some of the biggest challenges in democratizing big data analytics. Open Science is a critical component of both educational pedagogy and scientific research. With the increasing relevance of big data analytics, making these tools and resources available to the next generation of scientists and big data practitioners is crucial. Unfortunately, this is also extremely challenging.\nSecond: we aim to bring together big data practitioners from multiple backgrounds to discuss and establish the current state of affairs with respect to reproducibility in big data analytics and machine learning. Priorities change and constraints differ between researchers and developers in academia versus industry versus government; nonetheless, open science and big data are important to practitioners in each of these areas.\nThird: we aim to examine and propose possible routes forward to advance the continued integration of open science in big data analytics, putting tools, techniques, data, and documentation in the hands of researchers, students, and other big data practitioners. We will identify emerging trends in terms of open science best practices, and how these can be incorporated into current big data endeavors.\n\n\nWhat kinds of proceedings is the workshop accepting?\nWe hope to have two tracks: a full research paper session, and a student (short) paper session.\nPapers submitted to the OSBD workshop should, at the very minimum, have a significant Open Science component. This can take several forms, including but not limited to any combination of the following:\n\nUses free and open source packages\nOpenly available online, e.g. GitHub\nScripts are available to reproduce figures\nData are openly available e.g. dat, datahub.io\netc.\n\nWe are welcoming of original research into big data analytics, so long as there is an open science component.\nFurthermore, in the spirit of Open Science and reproducibility, we strongly encourage our presenters to incorporate live code demos, data walkthroughs, or some other hands-on activity into their talks.\n\n\nWhen is the submission deadline?\nThe deadline for workshop paper submissions is Sunday, October 25, 2016. This provides a 2-week buffer zone between the notification deadline of the main IEEE BigData conference, in case you want to repackage your conference submission for the workshop.\n\n\nWhere can one find more information about OSBD?\nWe have a website: https://osbd.github.io/\nThe website contains information about the topics around the workshop, the invited speakers who will be attending (keynote and panelists), the important dates around the workshop, and the instructions for formatting and submitting your paper to the workshop.\nWe are very excited about this workshop, and encourage you to submit! We look forward to seeing you in December!\n\n\nShannon Quinn, Co-chair\nJohn Miller, Co-chair\nSuchi Bhandarkar, Co-chair\nYi Hong, Co-chair"
  },
  {
    "objectID": "posts/2017-01-06-gameify-your-raspberry-pi/index.html",
    "href": "posts/2017-01-06-gameify-your-raspberry-pi/index.html",
    "title": "Game-ify Your Raspberry Pi",
    "section": "",
    "text": "My project over the 2016 holiday season was to take the Raspberry Pi 3 I’ve had sitting around the house idling for the previous nine months and turn it into a RetroPie-ified gaming emulator.\nRetroPie is a phenomenal bit of software that combines the work of several projects into a single package that was primarily designed to be deployed on Raspberry Pis, but which can also be set up on a regular ol’ PC. If you’re using a Raspberry Pi, it’s built on top of the Raspbian OS, so all the commands familiar to you will still be available.\nI have to say up front: the folks with RetroPie have done an amazing job. The documentation was excellent, and while I’ll walk through the installation and configuration here, with only one notable exception everything went exactly as prescribed.\n\nRetroPie\nAssuming you have all the needed hardware, installing the RetroPie OS on your Pi is pretty straightforward.\n\nUse some sort of imaging software to write the RetroPie image to an SD card (I used ApplePi Baker without any problems, but you could also just use the dd command in the Terminal, which directly moves bytes from one place to another).\nInsert the SD card into your Pi.\nBoot! I had a few false-starts that were the result of the SD card not being fully inserted, so nothing came up.\n\nIf all goes well, you should see the wonderful Raspbian logo.\n\n\n\nConfiguring RetroPie\nThe first thing you’ll need to do when RetroPie boots is to configure whatever controller you’re going to use. I went out and purchased some standard-issue Logitech USB gamepads and they seem to be functioning very well. You can also use your keyboard if you want for this, but it pretty much has to be something that can be connected via USB directly to the Pi.\nA few pointers in this step:\n\nTo skip a particular button configuration, just press and hold any button for a second.\nIf you make a mistake in configuring a button, you have to get all the way to the bottom before you can scroll back up and fix it.\nIt took me 5-6 tries to get this completely right. Just go back and do it again if you mess something up.\n\n\n\nEmulationStation\nOnce you successfully configure a gamepad, you’ll be taken to the main landing page: EmulationStation.\n\nWhere are teh gamez0rz?!1, you might be saying. Well, since ROMs are a legal gray area, you’re kinda on your own there. BUT! We have something else we need to do first anyway: set up a network configuration.\n\n\nNetwork Configuration\nIf you have a Raspberry Pi 2 or earlier, your network choices are limited to an ethernet connection. With the advent of Raspberry Pi 3s, suddenly wifi is an option! Turns out, I have a 3, and it is this wifi that caused me a couple hours’ grief until I figured out what was going on.\nGo to the wifi configuration in EmulationStation. You’ll be taken to a network selection screen–protip, the Pi 3 cannot connect to 5GHz networks–where you can select your wifi network.\nYou’ll then need to type in the wifi password. Provided you get it right, that should be it! You’re all set to go!\nIf you find yourself entering your password again…and again…and again…and again…and can’t figure out why it’s failing, read on.\n\n\nKeyboard Layout\nFor whatever reason, my keyboard was misconfigured–the layout was set to English (U.K.), resulting in some special characters being remapped. However, since the Raspbian password interface echoes stars * back, I had no idea I was typing a completely wrong password until I fired up a command prompt.\nLuckily, once you diagnose this issue, it’s fairly easy to fix. Raspbian has a utility built-in for changing your keyboard layout. You just have to navigate in EmulationStation to the option that gives you a command prompt.\nIn my case, once I had a working network connection, I just used SSH to perform any other operations on the Pi (e.g. uploading new games). The default username/password for RetroPie is pi/raspberry, which I suggest you change either from the EmulationStation main menu, or after your first SSH login.\n\n\nMoonlight on Raspberry Pi\nYou should have a working RetroPie at this point, so that’s cool!\nHowever, as awesome as the Pis are, they’re not exactly brimming with horsepower. As such, if you try running N64 or PS2 emulators, you may find as I did that anything requiring 3D shaders gets very jittery, to the point of unplayable. RetroPie has detailed configuration options for every emulator to help you get the absolute most out of your Pi, but there are some things a tiny CPU with an embedded stock GPU just can’t do, and one of those is render 3D shaders the way they were meant to be.\nSo what? Give up? Nay!\n\nMoonlight Embedded is the open source version of NVIDIA’s GameStream as used by the NVIDIA Shield. Basically, it’s a way of streaming the output of an NVIDIA GPU to another input, like a TV.\nIn this case, we’re basically turning our Pi into a glorified Chromecast–it’ll take the output from a dedicated GPU and stream it directly to the TV it’s connected to. Yes, this means you’re not actually playing games on the Raspberry Pi–you’re playing them on whatever computer has the GPU in it (oh and it has to be an NVIDIA GPU, GTX 650 or higher)–but all Moonlight requires is Raspbian, which is what RetroPie is built on!\nI found this installation guide perfect (make sure you download the latest versions of everything!), with the lone exception that, just before downloading and installing Moonlight itself, I also needed to install one more library:\n    sudo apt-get install libenet-dev\nInclude that with the other libraries installed just before Moonlight, and everything else should just work. The rest of the instructions take you through configuring the gamepads so they correctly send their input back to the GPU (so, y’know, you don’t have to sit at your computer while staring at your TV).\n\n\nHappy Fun Times!\nYes, lots of happy fun times. But also HappyFunTimes!\nIf you’re really into the party games, installing this gaming server may be the best thing ever. It’s a brilliant setup: gamepads are HTML5 canvases that run on smartphones, essentially mimicking controllers so everyone with a smartphone can play. The games range from stupidly simple to shockingly sophisticated. I may post configuration for that later, but I figure: if you’re building the Ultimate Gaming Pi, why not include that, too?\nAnd with that, I give you the results at our house:"
  },
  {
    "objectID": "posts/2016-05-16-autopost-running-twitter-summary-strava/index.html",
    "href": "posts/2016-05-16-autopost-running-twitter-summary-strava/index.html",
    "title": "A twitterbot for posting weekly running stats",
    "section": "",
    "text": "We runners (we’re a crazy bunch), for the most part, like our stats. How many miles do you log each week? Each month? How are your average race paces trending? Are your long runs both feeling good and getting faster?\nYes, we’re a little obsessed with our numbers.\nIt’s no surprise, then, that web services have popped up to help us aggregate some of these numbers. One of the most obvious is Garmin Connect, home for pretty much anyone who uses the Garmin GPS watches.\nAnother that I’ve used before is Daily Mile. However in recent months I’ve become frustrated enough with the site to leave entirely. By all accounts, no part of the site has been updated in years, and other alternatives are simply much more pleasant to use.\nUnfortunately, there was one crucial feature of Daily Mile I really liked: connecting it to your Twitter account to post weekly summaries of your recorded workouts. I liked it so much, in fact, I created a small web service to do the same thing, but for monthly summaries:\n\nObviously that service no longer exists, but ever since I’ve been wanting to get something similar up and running again. Especially now that I don’t even have weekly summaries anymore–just couldn’t stomach Daily Mile any longer–I wanted to take the opportunity (and the shiny new blog) to go through a step-by-step procedure of creating your own Twitter / Strava app for posting weekly summaries on Twitter of your running mileage!\n\nPreliminaries\nA few things you’ll need before we get started:\n\nPython 3.5\ntweepy (for interfacing with Twitter)\nstravalib (for interfacing with Strava)\npybot (shameless plug, but it will help)\n\nBoth Twitter and Strava use OAuth as their method of app authentication. These libraries just make it easier to interact with the services; they abstract a lot of the details of authentication and communication.\nBut in case you’re interested: Twitter’s docs and Strava’s docs.\n\nStep 1: Create a Strava app\nGo to your user settings and create an app that can interface with your account. Important pieces of information you’ll need later: Client ID, Client Secret, and Your access token.\nWe can test if it works. Fire up an IPython terminal, get your access_token, and run the following code:\n    from stravalib import Client\n    client = Client(access_token = access_token)\n    client.get_athlete()\nYou should see something along the lines of:\n    Out[]: <Athlete id=1234567 firstname=b'Firstname' lastname=b'Lastname'>\n\n\nStep 2: Retrieve last week’s data\nThe whole point is to get weekly mileage reports. Thankfully, the get_activities method in stravalib has an after optional parameter we can use to precisely tune what time interval we want.\nFirst, though, we need to create a timestamp that represents the 1-week time frame. If we assume this will only be executed on the day we want the summary–say, every Monday–then we need to tally up all the runs from the day before all the way back to the previous Monday, inclusive.\n    import datetime\n    current = datetime.datetime.now()\n    last_week = current + datetime.timedelta(weeks = -1)\n    after = datetime.datetime(last_week.year, last_week.month, last_week.day)\n    activities = client.get_activities(after = after)\nAssuming we run this chunk of code on a Monday, it should give us every Strava activity from the previous Monday up to the present.\nHowever, we’re not done yet. This includes everything–not just our runs, but any other activities that we recorded; yoga, weights, elliptical, and so on. We need to filter these out. We also need to filter out the edge case of any activities that have been recorded today, since we don’t want to include these in a report of last week’s activities!\n    l = [activity.id for activity in activities if activity.type == 'Run' \\\n        and a.start_date_local.day != current.day]\nOk, let’s pause and discuss what’s happening here.\nFirst, the most obvious: we’re looping through the activities generator we obtained in the last line of the previous step. Second, the if statement at the end filters out any activities that aren’t a run. Finally, the activity.id part out front says, we’re building a list of the unique IDs that identify each activity. The last part is our timeframe edge case: if we recorded an activity today, even a running activity, don’t include it.\nWhy are we only holding onto the IDs? It has to do with detail. Strava maintains a hierarchy of details available to users that vary with authentication, connection, etc. Simply put, when we request a list of activities, the default detail level is 2, which is “summary” level. However, some of the metrics we need–calories in particular!–require level 3, or “detailed”. To get this level of detail, we need to query for individual activities…one at a time.\nHence, a list of activity IDs! Now we can loop through the IDs, requesting details on each run and tabulating up the mileage and calories.\n    from stravalib import unithelper\n\n    mileage = 0.0, calories = 0.0\n    for activity_id in l:\n        activity = client.get_activity(activity_id)\n\n        # This is annoying; all the default distances are in meters! Luckily,\n        # stravalib comes with a nice unit helper utility to do the conversion.\n        distance = unithelper.miles(activity.distance)\n        mileage += round(distance.num, 2)  # Rounds to 2 sig figs.\n        calories += activity.calories\nThere you have it! In those two variables–mileage and calories–you have all the data you need to summarize your running workouts for the last week. Now we just need to post this information on Twitter!\n\n\nStep 3: Create a PyBot\nOk, time for a shameless plug: yes, I’m the pybot author. It’s still highly experimental, and largely uncompleted, but for our purposes it will suffice nicely as a barebones framework to interact with Twitter.\nClone the repo and follow the setup script to create a Twitter app and connect it to your account.\n    git clone https://github.com/magsol/pybot.git\n    cd pybot\n    sbin/create_pybot.py\nThat will walk you through the instructions for creating an app, generating OAuth credentials, and stubbing out your first pybot. Feel free to give it whatever name you’d like; for the purposes of this tutorial, I’ll assume we’ve named it artbot (don’t ask). Congratulations! You’ve created a twitter bot!\n\n\nStep 4: Customize the bot’s behavior\nOur bot is pretty simple: every Monday at some specified time, it will wake up, read all the prior week’s running activities, and post the summary before going back to sleep for another week.\nIt won’t be prompted by anything other than time. So the specific action override we’re looking for in PyBot parlance is on_tweet, and the interval we’ll use is tweet_interval. The latter is easy enough–a full week between tweets!\n    self.config['tweet_interval'] = 60 * 60 * 24 * 7\nBefore we go any further: does anyone see anything wrong with the above code snippet?\nI’ll give you a hint: imagine you started this bot on a Tuesday, instead of a Monday.\nYep, there it is. This interval we’ve defined is exactly 1 week, but it doesn’t account for when we actually START the bot. We need this to be a little more intelligent. If you want the posting to happen weekly every Monday, it shouldn’t matter when you actually start running the bot, right? It should be smart enough to figure out when it needs to post for the first time, then post weekly thereafter.\nA neat component of PyBot is that, in addition to giving hard time frames, you can also specify functions to compute the interval on-the-fly, subject to some other constraints that are dynamic (like on what day of the week you happen to fire up the bot).\nTo make things easy on us, we’ll use the datetime convention in the Python documentation for identifying individual days of the week. This tutorial assumes Mondays (which corresponds to 0), but you can use whatever value you want.\nWe need to store this as a configuration parameter in the bot.\n    # Put this somewhere in the bot_init() method\n    self.config['update_day'] = 0  # Corresponds to Monday.\nNow, we need a function to compute the interval between updates.\n    # Put this somewhere in the bot_init() method\n    self.config['tweet_interval'] = self._compute_interval\nWe’ve referenced an internal method we’re calling _compute_interval, as of yet undefined. Let’s go ahead and define it!\n    # Put this somewhere in the bot class declaration\n    def _compute_interval(self):\n        interval = 60 * 60 * 24 * 7  # The default interval; we'll start here\n\n        # Are we on the right day of the week?\n        now = datetime.datetime.now().weekday()\n        target = self.config['update_day']\n        if now == target:\n            return interval  # Nothing to do! Yay!\n\n        # If we get to this point, it means the index of the current day--\n        # as in, right when the code gets HERE--doesn't match the index of the\n        # day we've said we want to perform this update. So we need to do a\n        # little bit of work to compute that date.\n\n        if now > target:\n            # This is a hack, so the index of the CURRENT day will always be\n            # smaller than the index of the TARGET day.\n            now -= 7\n\n        # Essentially, all we've done is replace the 7 above with whatever\n        # it needs to be in order to get us to our target day.\n        return (target - now) * 24 * 60 * 60\nNow that our interval is in place, we’ll need to override the on_tweet action to do what we want whenever it’s called (which will be once each week on the day we’ve specified!). Remember, this method is called once we’ve hit our interval. So this is where it all comes together!\n    def on_tweet(self):\n\n        # First, pull in the stats from Strava.\n        current = datetime.datetime.now()\n        last_week = current + datetime.timedelta(weeks = -1)\n        after = datetime.datetime(last_week.year, last_week.month, last_week.day)\n        activities = client.get_activities(after = after)\n\n        # Second, filter by activity type and time frame.\n        l = [activity.id for activity in activities if activity.type == 'Run' and\n            a.start_date_local.day != current.day]\n\n        # Third, tabulate up the stats for mileage and calories.\n        mileage = 0.0, calories = 0.0\n        for activity_id in l:\n            activity = client.get_activity(activity_id)\n            distance = unithelper.miles(activity.distance)\n            mileage += round(distance.num, 2)  # Rounds to 2 sig figs.\n            calories += activity.calories\n\n        # Finally, use the stats to craft a tweet. This can be any format\n        # you want, but I'll use the example one from the start of the post.\n        tweet = \"My training last week: {:d} workouts for {:.2f} miles and {:d} calories burned.\".format(len(l), mileage, calories)\n        self.update_status(tweet)\nThat’s it! You have everything you need; now, just set the bot to run ad nauseum:\n    python artbot.py\nIt should run forever, sleeping for most of it but waking every week to post your summary. If you notice something isn’t working right, check the logs; they should specify if there are problems e.g. with permissions posting to Twitter, or connections hanging and disconnecting.\n\n\n\nConclusion\nThat’s all there is to it! There are obviously a lot of technical hurdles I largely glossed over–creating the apps for both Strava and Twitter can be a little more involved than the average person would like, and Python versions (especially 2.x vs 3.x) can wreak havoc on your code. I tried to be as reproducible as I could, though until Jupyter notebooks decide to play nice with Pelican (or maybe the other way around?) these code embeddings will have to suffice. Sigh.\nPlease feel free to leave a comment if you have any questions! I’ve also posted the bot in the examples folder in the pybot GitHub repository as artbot.py. Happy tweeting!"
  },
  {
    "objectID": "posts/2015-11-07-introductions/index.html",
    "href": "posts/2015-11-07-introductions/index.html",
    "title": "Introductions",
    "section": "",
    "text": "Well, this seems to be working. Kinda.\nI took some inspiration from Jake VanderPlas’ Pythonic Perambulations and opted for a similar route: Pelican as the backend blogging machine, github as the host, and (eventually) embedded Jupyter notebooks. Unfortunately, Jake’s addition to the Pelican plugins to allow Jupyter notebooks uses CSS that is, shall we say, unfriendly to Pelican themes outside of the Octopress default.\nSo that part is still incomplete. As are many of the links and some of the plugins. But ultimately, I’m hoping to use this to supplant my current pair of blogs that jointly detail what my plan for this space is: academia, general data science, and other musings.\nStay tuned!"
  },
  {
    "objectID": "posts/2017-05-31-summer-of-data-science/index.html",
    "href": "posts/2017-05-31-summer-of-data-science/index.html",
    "title": "Summer of Data Science 2017",
    "section": "",
    "text": "A few days ago, while I was still in Portland, OR enjoying the last few days of PyCon 2017, Renee (@BecomingDataSci) mentioned on Twitter that she might bring back the “Summer of Data Science” she kicked off last year.\nIn short: use the summer as an opportunity to learn and/or accomplish something data science-related in a goal-oriented, data-driven way.\nAs a tenure-track professor still yet to endure the 3rd-year review, pretty much everything data science related revolves around furthering my eventual tenure dossier. But the summers are still considerably more flexible for me to accomplish some of these goals than the fall or spring (I love teaching, but it’s all-consuming). I also have more than just myself–networks of collaborators and research assistants who can help me with these goals.\nSo without further adieu, I’d like to put forth my own Summer of Data Science, 2017 Edition:\n\nTeaching: 1360E improvements and 4360/6360 preparation\nI’m teaching an online course this summer, starting June 5 and continuing through the end of July: CSCI 1360E Foundations of Informatics and Analytics.\nOr, more colloquially, I just call it “introduction to data science.”\nIt’s one of the five courses I proposed in my first year (a story for another time, believe me…) and taught for the first time last summer, and in-person last fall. I’m teaching it again this summer and hoping to make some of the standard improvements, such as sprucing up slides and fixing autograder bugs from previous incarnations.\nMore pressing is CSCI 4360/6360 Data Science II, which has yet to have its own website. This course has never been taught, but is slated for Fall 2017.\nDid I mention there are already over 40 students registered, undergraduate and graduate combined?\nThis is supposed to be the “corner cases” of data science: lesser-used and/or more powerful analysis methods that aren’t necessarily the first tools or techniques you reach for in a given situation. This will include, but certainly not be limited to, concepts like randomized algorithms, semi-supervised learning, and backpropagation; techniques like out-of-core processing, distributed computing, and functional programming; tools like Julia, numba, and dask.\nBefore you ask: No. I’m not covering Spark.\nAt any rate, the conceptual framework is in place, but practically none of the architectural logistics have been established. So this will take up virtually all of my time for the remainder of the summer, once the following item is completed…\n\n\nResearch: NSF CAREER proposal\nThe advantage of this goal is it has a hard end date: July 19.\nThe disadvantage is that the NSF CAREER is the pinnacle of NSF early-career investigator awards. Its importance cannot be overstated; its je ne sais quoi is on display in the faces of new investigators when you mention it around them.\nI’ve connected with my university’s grant proposal officers, and am participating in a peer review process that involves two rounds of review: first with fellow CAREER submitees (as in, we exchange each other’s current drafts!), and second with the university grant officers.\nAn investigator can only make three submission attempts for the CAREER. The CAREER is only open for submissions once per year. If chosen, the investigator must still be un-tenured at time of award. These conditions place brutal limits on how long you can wait to start making submissions, and I already skipped submitting my first two years. Still, I have no illusions about the uphill battle I have, so my goal this first time around is simply to acquire feedback: from my peers, from my grants officers, and from the NSF reviewers.\n\n\nPersonal: Coursera’s optimization course\nI signed up to take Coursera’s Discrete Optimization course out of the University of Melbourne, taught by none other than Professor Pascal Van Hentenryck at Michigan and Dr. Carleton Coffrin, currently at Los Alamos.\nOptimization is pretty much the backbone of modern machine learning, and yet I have almost no formal training in it–beyond incidentally covering it in various machine learning and statistics courses as part of some broader topic in which it’s used. Oddly, the most rigorous instruction I received as a graduate student in optimization was a Cell & Systems Modeling course where we used various optimization techniques to find parameter values for population models.\nEspecially in conjunction with what I hope to teach in CSCI 4360 in the fall, this would be invaluable both professionally as well as personally."
  },
  {
    "objectID": "posts/2017-03-16-so-you-want-to-conduct-research-with-me/index.html",
    "href": "posts/2017-03-16-so-you-want-to-conduct-research-with-me/index.html",
    "title": "So, you want to conduct research with me?",
    "section": "",
    "text": "Deviation #1: This is wholly separate from the “should I get a Ph.D.” question. For that I would recommend one of numerous guides that ask all the right questions.\nDeviation #2: This is also wholly separate from how to succeed in a Ph.D. program, though there is some overlap. In general, there are some key items to consider and habits to build as you work your way through a Ph.D., and for those questions I highly recommend Andrej Karpathy’s excellent survival guide to a Ph.D., some of which may be mirrored here.\nNo, the specific question I’m addressing here is how to succeed in graduate research as one of my students. In order of somewhat-importance:\n\n1: Familiarize yourself with my research interests.\nThis may seem like a no-brainer, but you’d be surprised how many emails I get from individuals expressing profound interest in working with me, only to see they have i) no experience whatsoever in any of my interests (which isn’t necessarily a deal-breaker!), and ii) don’t appear to know what I work on (which is a deal-breaker). If you work with me, you’re going to do something at the intersection of bioimaging + distributed computing + biosurveillance; something that involves computer vision and machine learning in a public health setting.\nCheck out my Google Scholar record, my group’s GitHub repositories, and my lab website to get at least a basic idea of the sort of work I do, and to jog your thinking along the lines of what you might be able to contribute.\n\n\n2: Take into consideration the following expectations.\nWork hard, play hard, and be able to talk about both:\n\nI expect my students to develop into excellent scientific communicators. In practice, this means I want you to be able to talk about your work to experts in the same field, experts in different fields, and even non-scientists. I want you to get comfortable giving talks and writing papers. If English isn’t your first language, that’s ok! Just be aware this expectation may take more time than you think.\nI expect my students to participate in Open Science. In practice, this means I want you to publish all the code you write in public repositories, mirror all your papers on arXiv, participate in and contribute to open source projects, and maybe even contribute to a blog. Research is only interesting when you can pick through the code, explore the data, and regenerate the results. Know or learn how to use scientific notebooks, version control, wikis, and even containers!\nI expect my students to demonstrate a promising slope of accomplishment. Put simply, I recruit for potential (slope), not experience (y-intercept). You don’t have to know everything about machine learning, statistics, and linear algebra, and be an expert programming in Python just to be able to work with me. However, I do expect that you will be able to pick up these skills very quickly.\nI expect my students to take ownership of their projects, pushing the envelope of what is known and beyond what even I would suggest.\n\nSpeaking of which…\n\n\n3: Prepare yourself to be a self-sufficient researcher.\nThis does NOT mean I expect you to do everything! The whole point of being a student is that you have a mentor, a supervisor who is (in theory) more experienced than you, from whom you can learn. It is indeed my job to guide you and teach you what I know, and I will most certainly do that.\nThis DOES mean that, by the end of your time here, I want you to be the expert on your project! You should take the project and run with it, rather than wait for me to tell you what to do next. Build your intuition about the problem through “moving fast and breaking things,” to use the tech startup parlance. Ask forgiveness rather than permission. Insert witticism here that basically says KEEP TRYING THINGS.\nEven as you start your project, you may come across roadblocks that I don’t have an answer for. That’s how research works: we’re pushing the limits of our collective knowledge! I may have some intuition from previous problems, and I’ll certainly share that, but asking me “What do I do next?” is going to frustrate both of us very, very quickly.\nHere’s another example: the levels of data science classes. You don’t have to be anywhere above Level 2 when you arrive (maybe you’re still working your way through Level 1!), but I expect you to work through Level 7 by the time you leave.\n\n\nIf you’ve made it this far, and are still interested…\n…then I encourage you to reach out to me and express your interest! I have just a few final pieces of advice:\nFirst, read this brief Twitter thread and this brief Twitter thread as well.\nNext, do NOT send me\n\na form email with a 10-page summary of your accomplishments (as I said, I don’t hire for y-intercept, I hire for slope)\nan email with misspellings or grammatical errors (recall that I emphasize good scientific communication skills)\na message that starts with “Dear Ms Quinn” (first, I’m a doctor; second, I’m a guy)\n\nI won’t respond.\nFinally, write an email that’s 3-5 sentences at most, with a specific mention of the work you find interesting and want to pursue further. For extra credit, mention some kind of improvement or extension to the work that you came up with on your own. This tells me a lot of things all at once, most important of which is that you’re serious about wanting to work with me.\nBecause if you can handle graduate school…\n\n…I really do believe you can handle anything."
  },
  {
    "objectID": "posts/2023-01-14-year-that-almost-was/index.html",
    "href": "posts/2023-01-14-year-that-almost-was/index.html",
    "title": "2022: A year in review",
    "section": "",
    "text": "Warning\n\n\n\nThere’s going to be language in this post. Consider skipping if you prefer your blog posts merely warmed and with cream; this one is straight out of the pourover.\n\n\n2022 was a fucking shitshow.\nForget the transition out of academia that fell flat on its fucking face in the final mile1. Forget the actual running that went from runway to runway, gunning the engine and building some speed on occasion but without ever quite taking off. Forget 2022’s parting shot of giving our entirely family COVID within days of the year’s end (save our daughter, miraculously and thankfully) when we’ve been so diligent for three fucking years. Forget that we’ve been pretty much the only ones still wearing masks on planes and at our daughter’s daycare, as if “I’ve decided to get on with my life” somehow makes the pandemic over simply by willing it; as if it’s impossible to “get on with life” while also taking basic fucking precautions that hurt no one and help everyone.\nAll of that was rage-inducing beyond words and each deserve their own posts to really break down. But the real story of my 2022 was of burnout that I foolishly thought I could outrun.\n\nBurnout dances to its own tune\nI’ve been burned out for quite awhile now; realistically, there are likely roots going all the way back to graduate school, but things really kicked into gear with the tenure-track position and the stress it brought.\nThe pandemic didn’t create anything new in this regard, but it did act as a force multiplier, frying what little bandwidth I had left and forcing me to confront just how toxic my working circumstances were.\nThis was all complicated by the birth of my daughter, happening at the same time as my promotion and tenure process, wrapped up in some of the most gratuitous displays of academia’s utter disdain both for growing families and, it turns out, straightforward public health guidelines around pandemic mitigation strategies.\nSuffice to say, when I landed a job at Quansight in mid-late 2021, I jumped at the possibility of an exit strategy. Admittedly, I jumped way, way too soon: I was worried of losing the opportunity if I waited (irony), I didn’t have anything approximating a concrete exit strategy from academia (haste), and I thought I could hold down two positions long enough to gracefully exit (fucking LOL).\n\n\nWriting on the wall\nEven when I formalized a sabbatical beginning August 2022–an entire eight months after I started working full-time at Quansight–I was so far beyond any kind of discernible burnout threshold that it’s a miracle I lasted another three months before leaving.\nThrough all of 2022, my burnout was so severe that entire consecutive weeks would go by where I napped daily just to make it through the day. I was besieged by migraine headaches at the base of my neck that radiated down my legs; they wouldn’t respond to ibuprofen, and made literally any activity impossible. By the fall, I was having these migraines 3-4 days a week. But the coup de gr\\(\\hat{a}\\)ce, the real cherry on top of it all was how, on a given work week in 2022, my upper bound for productivity was probably about 25% efficiency.\nYes, I typed that correctly: on a given 40-hour work week, I at my peak could probably crank out about 10 hours of real, actual work. Most of the time, I was operating at some sigma less than that. I’ll take my promotion now.\nThe weirdest thing about this level of burnout probably answers the obvious burning question following that previous revelation: I had no idea this was all related, or that there was even really a problem. The headaches were annoying, the naps made things bearable (in a sense), and surely I was just still getting the hang of things and I’d soon start churning out productive weeks? After all, I just had to hold out until I could sever all ties to academia in… 6-8 more months.\nAbout that.\n\n\n\nMidjourney did a pretty bang-up job when I asked for a tweaked version of “this is fine”.\n\n\nTo address the (one of many, I suspect) elephant in the room: wasn’t I on sabbatical from my academic role? Yes, dear reader, I was. And it was still requiring at least a handful of hours from me, on a good week. Some sabbatical.\nOh yeah, October. Our entire family got RSV in the second half of October. Didn’t I mention that earlier? No? Yeah, that was an entire week where all three of us were sick (so no daycare for kiddo), followed by a second week where my daughter and I developed secondary bacterial infections! Great for the aforementioned productivity!\n\n\nNo options given\nIf there’s anything positive to be said about burnout, it’s this: eventually, it gives you no choice. You reach a point where it’s physically, mentally, emotionally, spiritually, ecumenically, and grammatically impossible to keep moving forward.\nWithout going into detail2, that point came just before Thanksgiving. I left Quansight. I took the entire month of December off. And hooooooly shit y’all, but that month was, honest-to-goodness, maybe the first time in over a year I not only stopped digging the burnout hole deeper, but actually started filling it back in. I can’t describe the feeling of my brain lighting up and making connections and actually functioning–I spun up our household Mastodon instance in that time3! I actually recovered a bit, holy fuck.\nAnd only then did I slowly begin to realize how absolutely fucked I’d been the entire year up until that point. How the fuck did I expect to be able to continue down this road for another six to eight MONTHS?\nIt seems ridiculous–laughable, if it didn’t border so close to outright dangerous–but only in hindsight.\n\n\nLight even in darkness\nMy 2022 was defined by my burnout… but also by my realization of its full extent. And, in doing so, by the start of me turning things around.\nBeyond burnout, there were some other events and accomplishments in 2022 that were cause for celebration by themselves.\nMy wife quit the job that was stifling her and is now pursuing her dreams as a full-time fiction writer. This was one of the very first things I learned about her all those years ago when we met4, and the fact that she’s living it now–and all the challenges and anxieties and freedoms and blessings it comes with–makes me prouder than I’ve been of anyone in years.\nSpeaking of my wife and her writing, she finished a complete draft of the book she’s been working on for the past bit. I finished reading it a week or so ago, and I fucking love it. I may be a little biased, but I also read 30 books last year according to my StoryGraph5, so I have at least an inkling of what I’m talking about. She’s an amazing writer; you should buy it when it’s published!\nMy daughter is rapidly turning into the most fiery and powerful person I could have ever hoped she would be. It’s exhausting and enthralling and I’m so freaking happy to be her dad.\nWe did, in fact, solve some of the flooding problems around our house6. We had extensive work done on the back and front yards, and I can proudly say that our front yard is virtually problem-free and our backyard is… better. Obviously there’s more to be done, but in exchange for a lot of money there was a good amount of progress toward eliminating the literal rivers that swirl around our house during a downpour.\nFor all my breathless pearl-clutching around my running failures, 2022 was, by any reasonable measure, an improvement over 2021, and those improvements should be celebrated… even if they aren’t the impossible gains I wanted.\nStrava has a fitness estimate tracker; it’s obscenely biased in ways I can’t begin to wrap my head around, but it seems like a moderate-to-low variance estimator, so it provides a decent relative intuition on fitness over time. And I have to say: given my 2022, it’s halfway ok.\n\n\n\nYeah, my fitness hasn’t been great the past 6 months, but it’s held relatively steady. Given 2022, that’s an accomplishment by itself.\n\n\nSpeaking of running, my total mileage improved year-over-year. I also ran my first half marathon since before COVID7. It was the slowest half I’ve ever run (by a lot), but I finished. I also ran a handful of other races last year, which helped remind me that racing is a lot of fun–which helped remind me that running is a lot of fun.\n\n\n\nYear-over-year total mileage run. More mileage with fewer runs indicate longer runs.\n\n\nI spun up an entire Mastodon instance, Casa Quinnwitz, on my homelab Raspberry Pi cluster (shout-out to the Mastodon dev team, which is full of super kind and patient folks). It’s pretty much my new socialz home, since Twitter has been rendered all-but-unusable with Tweetbot and all the other third-party clients going belly-up. I kind of love the vibe there; feels very longer-form-proto-Twitter with a pre-Eternal September thing going.\n8\n\n\nWhat’s next?\nI’m back in the academy this semester, teaching my data science practicum course while guiding a half dozen of my remaining students to graduation in May. I have a weekly structure in place that balances my obligations to the academy with my needs for continuing rest, exercise, and my own fun shit. I’m still feeling the after-effects of COVID but am hoping that will taper off as I get back into running again, training for a half marathon in April and multiple shorter races before then. I have a concrete exit strategy from academy, and a timeline of events and milestones to accompany it.\nCrucially, I’ve also adopted a new strategy for handling work-related items coming my way: NO.\nNo, I’m not taking new students.\nNo, I can’t review for your conference/study section/journal.\nNo, I’m not open to new research collaborations.\nNo, I can’t sit on your [insert topic here] panel9.\nNo, I’m not going to be on that committee.\nNo no no no no. No.\nIt’s a really hard fucking thing for a lifetime people-pleaser to set up boundaries, but for me in this moment, it’s both a matter of mere survival as well as a pathway to doing what I really love.\nHere’s to a 2023 of healing. Much love to all of you 🥂\n\n\n\n\n\nFootnotes\n\n\nYes, there will be a future blog post on this. No, I’m not going to expound any further on it here.↩︎\nAsk if you really want to know.↩︎\nDon’t worry, I have an entire series of blog posts (six, I think?) planned to go over it in gross detail. Stay tuned!↩︎\nJust about 17 years ago! 😱 ↩︎\nDitch Goodreads!↩︎\nThis one’s for you, Mom and Dad 🎄✉️ ↩︎\nThe last half I ran was late 2019, almost a 3-year gap!↩︎\nNow I’m just fucking with you. Also I love footnotes, so expect to see a lot of them on every blog post.↩︎\nThough I did get invited to one on ChatGPT, and wow the temptation to say “yes” and then just burn the whole place down is overpowering… I’ll have to think about it.↩︎"
  },
  {
    "objectID": "posts/2022-12-13-pardon-the-dust/index.html",
    "href": "posts/2022-12-13-pardon-the-dust/index.html",
    "title": "Pardon de dust (2)",
    "section": "",
    "text": "Why hello there. You may have noticed that things look a little bit different around here.\nI’m in the process of migrating my blog off Pelican and on to Quarto.\nWhy, you ask? Honestly, I got tired of having to fix a broken plugin or failed git push every. single. time. that I finished writing out a blog post. It gave an already-infrequent task an even higher activation threshold, and during a stretch where spare bandwidth was an incredible luxury.\nIn short: I’d really like to blog more often, and from what I’ve seen, Quarto will help me do just that. It definitely requires some extra bandwidth to get things migrated over, but fortunately: I have some of that at the moment :)\nStay tuned! In the meantime, please enjoy the above recent photo of Clover helping us put up our tree."
  },
  {
    "objectID": "posts/2016-02-02-nsf-crii-reviewer-feedback/index.html",
    "href": "posts/2016-02-02-nsf-crii-reviewer-feedback/index.html",
    "title": "Reviewing a reviewed grant’s reviews",
    "section": "",
    "text": "Rejection in any context sucks. As a new faculty trying to prove himself (to himself, but most importantly, to his tenure committee), I particularly hate rejections in the context of grant proposals and paper submissions. The “nice” thing about the latter rejection is that it’s marginally easier to resubmit a manuscript elsewhere; there are more journals and conferences than ever, especially in computer science.\nLast September, I submitted a proposal in line with NSF’s 15-569 program solicitation: the Research Initiation Initiative in Computer and Information Science and Engineering.\nInsert jokes involving “initiation initiative” imagery here (sorry, I don’t have an avalanche of Advil).\nSomewhat unsurprisingly but nonetheless disappointingly, I received notification today that the proposal was rejected. I say unsurprising because the proposal itself was incredibly rushed; we hit “submit” about two minutes before deadline. It was my first major grant proposal, so there were bound to be some mishaps.\nSo: let’s get into the reviewer feedback!"
  },
  {
    "objectID": "posts/2016-02-02-nsf-crii-reviewer-feedback/index.html#conclusions",
    "href": "posts/2016-02-02-nsf-crii-reviewer-feedback/index.html#conclusions",
    "title": "Reviewing a reviewed grant’s reviews",
    "section": "Conclusions",
    "text": "Conclusions\nFor the vast majority of weaknesses, their immediate impetus can be summed up thusly: I started writing the grant the day before deadline. That’s a no-no, and contributed directly to the panicked flurry at the end and lack of detail in the proposal itself. Starting earlier will solve these problems.\nThe last point that genuinely concerns me, though, is the first reviewer’s comment on differentiating myself from my graduate advisors. It’s true that this proposal took work I started as a graduate student, and I agree that it’s important to build an independent research group such that I don’t have to rely on my graduate advisors for funding.\nBut I suppose I could boil this down to time constraints as well: given more elbow room, I could have provided a more detailed development plan that clearly differentiated itself from work done before. Rather than relying on the “we propose to extend…” mantra, I could have delved into much greater detail on the methods I’d wanted to use and why their application would be novel.\nSo there you have it: a review of grant reviews. Nothing left to do here but incorporate the feedback and push out more effective proposals in the months to come!"
  },
  {
    "objectID": "posts/2023-01-23-mastodon-home-network-topology/index.html",
    "href": "posts/2023-01-23-mastodon-home-network-topology/index.html",
    "title": "Mastodon, Part I: My home network topology",
    "section": "",
    "text": "This article is part of a series about installing and configuring a Mastodon instance on a cluster of Raspberry Pi computers running k3s. To go back to previous articles in the series, try any of the links below:\n\nIntroduction\nPart I: My home network topology (this post)\nPart II: The Mastodon Helm chart\nPart III: Configuring and installing prerequisites\nPart IV: The waking nightmare that is Let’s Encrypt\nPart V: Actually installing Mastodon\nConclusions\n\nAdmittedly, this post in the series will be less “here’s how to spin up your own instance” than “here’s why I had so many problems that are specific just to me”, so if you have a solid handle on your own home network topology and/or are a DNS savant, feel free to skip this post.\nFor the rest of us mere networking mortals, let me walk you through a network hiccup that has been stymieing my attempt at an honest-to-goodness homelab kubernetes cluster for the better part of the last couple years.\n\nThanks a lot, Bezos\nDuring the height of the pandemic in summer 2020–and somewhat in preparation for the birth of our daughter–I performed some long-overdue home network upgrades. This included 1) switching over to fiber internet, and 2) upgrading to a mesh network, rather than the janky router-plus-extender setup that honestly never really worked all that well. The router itself was already 5+ years old (purchased when we first moved into the house), so it needed an upgrade anyway.\nAt the recommendation of some friends, I went with eero. Honestly, I’d still recommend it: it’s been rock-solid, with no problems between my Apple devices (MBP, iPad Pro, and iPhone), my wife’s Android+Windows devices (Surface, Pixels), and our myriad internet-of-sh!t devices strewn throughout the house. Literally zero problems, which I honestly can’t even say for our old router.\n…with, I suppose, one exception: putting the eeros into bridge mode.\n\n\nBridge mode is not what you think it is\nAs far as I’m aware, across all devices–from ISP modems to VirtualBox to Docker to home routers–the term “bridge mode” has a common intuition: it means the device that has been placed into bridge mode relinquishes any intelligent packet organizing methods it may have been using before and simply acts as a simple “bridge” between whatever entities it is connected to. As such, it doesn’t really care what those entities are, it just passes traffic between them.\nNot so with eeros.\nTo illustrate, here is what my home network looked like before starting any of this craziness.\n\n\n\nWhy yes, I did configure MoCA for my home mesh, thanks for asking.\n\n\nThis setup worked, except… see that Raspberry Pi cluster in the bottom left? That’s where I installed k3s for tinkering, but “tinkering” is pretty much all anything amounted to, because eeros have an odd limitation when it comes to port forwarding: they don’t allow IP-based service forwarding, or at least, service forwarding that is based solely on IP addresses; they also require MAC addresses. When dealing with ephemeral services created by kubernetes clusters on floating IP ranges, there aren’t associated MAC addresses with these services, which makes it impossible to use software-based load balancers like metallb, and therefore impossible to deploy multiple services that use similar configurations–like, say, multiple websites: Mastodon and something else.\nSo the first thing I had to do, were I to run my own instance, was to figure out a network configuration that would allow me to do IP-based port forwarding. I recalled that my old router had this ability, so I figured: let’s just pull that out of retirement and use it purely for port forwarding!\nFollowing was my first attempt. Note the major changes: the router has taken the place of the “main” eero as connecting directly to the ISP modem, and what was formerly the “main” eero is now just a wireless access point, connected to nothing except the router.\n\n\n\nMy first attempt at putting the eeros in bridge mode and un-retiring my old router.\n\n\nSuffice to say, this didn’t work. I mean, it kind of worked: the Raspberry Pi cluster was getting the right traffic forwarded to it. But seemingly random devices on the network would, suddenly and for no reason I could discern, disconnect and refuse to reconnect unless the entire network was rebooted.\nNot exactly a tenable situation. So I reverted to the previous configuration where the eeros were in charge while I tried to figure out what was going on.\nFast forward about 8 months. I finally, finally found this post on Reddit (because why would eero have it in their technical support documents?) from a couple years ago, with this critical element:\n\nApparently, even in bridge mode, there has to be at least one eero that can “see” the entirety of your home network (i.e., all incoming traffic should pass through it).\nThis… made absolutely no sense to me, as it directly contravened my understanding of “bridge mode”. But I went with it, and retooled my network to follow this new bit of information.\nHere was my next attempt. Like my previous attempt, the router is connected directly to the ISP model. However, unlike my previous attempt, what was the “main” eero in the original configuration–while still connected to the router–now sits between the router and the entire rest of the home network, rather than just the router and… whatever devices deign to connect wirelessly to it.\n\n\n\nMy final attempt at a network reconfiguration, this time putting one eero ahead of the rest of the home network.\n\n\nThis configuration, while still strange to me, officially works–or at least, it’s worked for the past few months with no issues.\n\n\nLoad balancers: green\nWith the issue of the k3s Raspberry Pi cluster being able to receive incoming traffic from the wider internet effectively resolved, I could now undertake the process of installing a Mastodon instance on the cluster with gusto.\nIn the next post, we’ll look at the Mastodon Helm chart in all its glory. Stay tuned!\n\n\n\n\nCitationBibTeX citation:@online{quinn2023,\n  author = {Shannon Quinn},\n  title = {Mastodon, {Part} {I:} {My} Home Network Topology},\n  date = {2023-01-23},\n  url = {https://magsol.github.io/posts/2023-01-23-mastodon-home-network-topology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nShannon Quinn. 2023. “Mastodon, Part I: My Home Network\nTopology.” January 23, 2023. https://magsol.github.io/posts/2023-01-23-mastodon-home-network-topology/."
  },
  {
    "objectID": "posts/2023-01-20-mastodon-instance-k3s-rpi-introduction/index.html",
    "href": "posts/2023-01-20-mastodon-instance-k3s-rpi-introduction/index.html",
    "title": "Running your own Mastodon instance on a Raspberry Pi k3s cluster",
    "section": "",
    "text": "Introduction (this post)\nPart I: My home network topology\nPart II: The Mastodon Helm chart\nPart III: Configuring and installing prerequisites\nPart IV: The waking nightmare that is Let’s Encrypt\nPart V: Actually installing Mastodon\nConclusions\n\nAs I mentioned in my previous post, I spent the last month of last year spinning up my own Mastodon instance (which you can check out!) I’ve had a Raspberry Pi cluster of 5x Pi 4B modules sitting around for the better part of the last two years1. I’ve occasionally toyed with installing k3s and experimenting with various bits and pieces of kubernetes, but never actually deployed something I found useful.\n\n\n\nDon’t worry, they’re well protected.\n\n\nDon’t get me wrong–I do want to go over how I installed and configured k3s on my Raspberry Pi cluster. It’s remarkable how quickly this area of things moves, and the guides I was working off for this were all 2-3 years old, and remarkably outdated. So a more recent tutorial on that basic component would definitely have its niche, I think.\nBut first, I want to kick off a series of posts detailing how I got a Mastodon instance running! It took a surprising amount of wrangling, so I wanted to share my experience before I forget the tiny details.\nWhy would you be interested in this series? Aside from the usual rough-and-tumble of installing anything on kubernetes via helm, the two parts that took the greatest amount of time and effort were 1) getting the Mastodon dependencies to work, and 2) wrestling Let’s Encrypt into submission. That first point is interesting because while Mastodon itself works just fine on ARM, the default images included in the Mastodon helm chart for the dependencies do NOT, and so it won’t work out-of-the-box on a Raspberry Pi cluster. And the second point was just plain rage-inducing, and I’d love for folks to not have to experience that.\nI’ll fill in the links as I write the posts, so that you can bookmark this page if you’d like, though I’ll make sure to include the links on the other posts as well as I write them. In the meantime, don’t hesitate to drop any pointers / corrections / questions!\n\n\n\n\nFootnotes\n\n\nYes, I did indeed purchase them two years ago. Would’ve been nigh impossible to get them at any time in between 😬↩︎\n\nCitationBibTeX citation:@online{quinn2023,\n  author = {Shannon Quinn},\n  title = {Running Your Own {Mastodon} Instance on a {Raspberry} {Pi}\n    K3s Cluster},\n  date = {2023-01-20},\n  url = {https://magsol.github.io/posts/2023-01-20-mastodon-instance-k3s-rpi-introduction/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nShannon Quinn. 2023. “Running Your Own Mastodon Instance on a\nRaspberry Pi K3s Cluster.” January 20, 2023. https://magsol.github.io/posts/2023-01-20-mastodon-instance-k3s-rpi-introduction/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Shannon Quinn. I’m an associate professor in the School of Computer Science and Department of Cellular Biology at the University of Georgia. Which is ironic at some level, considering my undergraduate degree in Computer Science came from Georgia Tech (GO JACKETS).\nMy interests are pretty broad, but the best summary I can give is that I enjoy hacking at large-scale data science problems. Recently I’ve been getting into DevOps, and ways of operationalizing machine learning; reproducibility is very important to me, and anything that helps make the hidden work of preprocessing, pretraining, and even precoding explicit is a good idea in my book. Perhaps unsurprisingly, I’m a huge proponent of open source development, and a big believer in open science. I try to make tools, data, and course materials all available with open source licenses, and try to give back to the open source communities whenever and however I can.\nIn my copious (lulz) free time, I enjoy board gaming, video gaming, and anything that involves working up a sweat: baseball, football, basketball, racquetball, cycling, hiking, and especially running."
  }
]