{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently I've been working with some of my students on a paper for the [SciPy Conference](https://scipy2018.scipy.org/) related to our OrNet project. Part of this involved designing our own [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) (GMM). In doing so, I somewhat accidentally-on-purpose stumbled upon the beauty of `einsum`. So I wanted to briefly motivate its use (expanding upon the [short use-case I wrote on my research group's blog](https://medium.com/the-quarks/an-einsum-use-case-8dafcb933c66)) and show the performance results of re-implementing iterative computations using `einsum`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Madness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the critical steps of this algorithm is to evaluate the probability of any given point $\\vec{x}$ under the model that we've learned so far (the current values of $\\vec{\\mu}$ and $\\Sigma$. In higher dimensions (as in, higher than 1), this has a pretty tricky formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\vec{x} | \\vec{\\mu}, \\Sigma) = \\frac{\\exp{(-\\frac{1}{2}(\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1} (\\vec{x} - \\vec{\\mu}) )}} {\\sqrt{(2 \\pi)^2 \\det{(\\Sigma)} }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's...messy. The things that make it particularly tough to efficiently evaluate include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - the matrix multiplication in the numerator\n",
    " - the inversion of $\\Sigma$ in the numerator\n",
    " - the determinant of $\\Sigma$ in the denominator\n",
    " - **the fact that this has to be computed for *every* $\\vec{x}$ (thousands), under *every* combination of $\\mu$ and $\\Sigma$ (often a few dozen), at *every* iteration of the algorithm (often a few dozen)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! That's not all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also critical to the GMM are the *update* steps: re-estimating the current values of the parameters so they (hopefully) better fit your data. This happens *a lot*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multidimensional wonderlands, the updates of the parameters look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{\\mu}_{ki} = \\frac{1}{N_k} \\sum_{n = 1}^N r_{kn} \\vec{x}_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Sigma_{ki} = \\frac{1}{N_k} \\sum_{n = 1}^N r_{kn} (\\vec{x}_n - \\vec{\\mu}_{ki})(\\vec{x}_n - \\vec{\\mu}_{ki})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first equation for $\\vec{\\mu}_{ki}$--the $k^{th}$ component (one of the 50-100) at the $i^{th}$ iteration (often at least a few dozen)--is a \"fairly\" easy multiplication of each data point $\\vec{x}_n$ by some number $r_{kn}$, all summed up and scaled by $\\frac{1}{N_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second equation is a lot harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every $\\vec{\\mu}_{ki}$ we have a corresponding $\\Sigma_{ki}$. Which, remember, is a full matrix. This involves taking the $\\vec{\\mu}_{ki}$ we computed in the first equation, and computing outer products with every data point $\\vec{x}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Outer product\" is a fancy name for \"matrix multiplication\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring on the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of course, I initially implemented these two core operations in the way that made the most sense: using `numpy` functions and Python comprehensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For computing $P(\\vec{x})$**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def px1(X, mu, sigma):\n",
    "    det = sla.det(sigma)\n",
    "    inv = sla.inv(sigma)\n",
    "    p = np.array([(x - mu).dot(inv).dot(x - mu) for x in X])\n",
    "    n = 1 / ((((2 * np.pi) ** 2) * det) ** 0.5)\n",
    "    px = np.exp(-0.5 * p) * n\n",
    "    return px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **for re-computing $\\mu$ and $\\Sigma$**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def musig1(X, r):\n",
    "    N = r.sum()\n",
    "    mu_next = (r[:, np.newaxis] * X).sum(axis = 0) / N\n",
    "    \n",
    "    s = np.zeros(shape = (X.shape[1], X.shape[1]))\n",
    "    for x, ri in zip(X - mu_next, r):\n",
    "        s += (np.outer(x, x) * ri)\n",
    "    sigma_next = s / N\n",
    "\n",
    "    return mu_next, sigma_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were computational *bears*. In `px1`, the line starting `p = ...` is a *huge* computational cost: doing two dot products inside a loop over all the data points $\\vec{x}_n$ is really, really expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in `musig1`, the operation to compute the next value of $\\Sigma$ involves computing the outer product of every $\\vec{x}_n$ subtracted by the current value of $\\vec{\\mu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffice to say, when it came time to actually run the algorithm, it shouldn't come as a surprise that, with a dataset of roughly 130,000 points in 2 dimensions, each iteration of our algorithm took about **52 seconds**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the *vast majority* of which was spent computing dot products!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![before](/images/einsum/before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half of a single iteration was devoted to dot products in the `px1` function above. That's *bad*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for grins, let's time everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random(size = (130000, 2))  # To simulate the data I was working with.\n",
    "mu = X.mean(axis = 0)  # Compute an actual mean.\n",
    "sigma = np.array([[5, 0], [0, 5]])\n",
    "r = np.random.random(130000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 ms ± 6.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit px1(X, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847 ms ± 5.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit musig1(X, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a full second of computation just for 1 of the Gaussian components (i.e., one pair of $\\mu$ and $\\Sigma$). Crank that number of components up to 50 and, well, there's your ~50-second runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was admittedly at a bit of a loss here, considering I was already using NumPy constructs. But at the same time, these were highly structured linear algebra operations; there had to be something more efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `einsum` to the Rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: I should highlight that I *didn't immediately land on this*. It took some Googling around and some sitting and thinking before I figured out how this could be used. Then it took some more trial-and-error to actually get it to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But work, it did. Holy crap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.einsum` is an implementation of the [Einstein Summation](http://mathworld.wolfram.com/EinsteinSummation.html), which is a super-fancy term for a cool (but admittedly tricky-to-grasp) shorthand notation for dealing with multiplications and summations of vectors, matrices, and tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't rehash the basic examples I've found across the web that helped me understand how this works, but I will give special emphasis to two articles:\n",
    "\n",
    " 1. [Einstein Summation in NumPy](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/), and\n",
    " 2. [A basic introduction to NumPy's einsum](http://ajcr.net/Basic-guide-to-einsum/)\n",
    " \n",
    "I found #2 particularly accessible, but I'd recommend both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took some doing, but I managed to rewrite both equations using `einsum`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def px2(X, mu, sigma):\n",
    "    det = sla.det(sigma)\n",
    "    inv = sla.inv(sigma)\n",
    "\n",
    "    ###\n",
    "    p = np.einsum('ni,ji,ni->n', X - mu, inv, X - mu)\n",
    "    ###\n",
    "\n",
    "    n = 1 / ((((2 * np.pi) ** 2) * det) ** 0.5)\n",
    "    px = np.exp(-0.5 * p) * n\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def musig2(X, r):\n",
    "    N = r.sum()\n",
    "    mu_next = (r[:, np.newaxis] * X).sum(axis = 0) / N\n",
    "\n",
    "    ###\n",
    "    sigma_next = np.einsum(\"ni,nj->ij\", (X - mu_next) * r[:, np.newaxis], X - mu_next) / N\n",
    "    ###\n",
    "\n",
    "    return mu_next, sigma_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `musig2` use actually ended up being a straightforward application of the \"each row of $A$ multiplied by $B$\" rule at the end of the #2 link above, with the slight wrinkle of making the *first* axis, `n`, the one that is summed over in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one in `px2` was a lot harder, and for a time I wasn't convinced it could be done. At one point I was able to basically split the operation in two `einsums` that were more or less identical to the one in `musig2`; it basically involved a repeated application of \"multiply each row of $A$ by $B$\". But I kept hunting around the web, particularly for examples of multiplying many matrices together, and eventually managed to figure this one out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It boiled down to basics of how `einsum` works: I knew the output had to be a list of numbers `n` long, and that the first and third inputs were identical (`ni`). The key was: if everything had 2 axes going in, but only 1 axis was coming out, then all three inputs needed to share one axis that could be summed over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With great power blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's run our examples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.82 ms ± 32.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit px2(X, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.37 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit musig2(X, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Going from a list comprehension of calls to `numpy.dot` to a single `einsum` decreased runtime of a single iteration from **300ms** to **7ms**, a speed-up of over 40x.\n",
    " - Going from a loop of `numpy.outer` products to a single `einsum` decreased runtime of a single iteration from **847ms** to **7.5ms**, a speed-up of over 110x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I re-ran `cProfile` to see what my performance gains were, and holy *crap* Batman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![after](/images/einsum/after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only was my total runtime down from ~52 seconds per iteration to a mere **1.5 seconds per iteration**, but the most time-consuming block of my code was now `einsum` itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, I wasn't going to argue with a full *order of magnitude* reduction in runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SciPy paper is still under review, so feel free to [head over there and check out](https://github.com/scipy-conference/scipy_proceedings/pull/396) how we're putting GMMs to use for bioimaging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
