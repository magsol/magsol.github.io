{
  "hash": "78a04d2ab5f9c930f1d01e73c8d39dfd",
  "result": {
    "markdown": "---\ntitle: An einsum use-case\ndate: '2018-06-19 16:58:02'\ncategories:\n  - professional\n  - python\ncitation:\n  url: 'https://magsol.github.io/2018-06-19-an-einsum-use-case'\nformat:\n  html:\n    code-fold: true\nimage: kOqM5.png\nimage-alt: 'A diagram of Einstein summations. Source: https://stackoverflow.com/questions/26089893/understanding-numpys-einsum .'\nauthor: Shannon Quinn\n---\n\n![](kOqM5.png){.preview-image fig-alt=\"A diagram of Einstein summations. Source: https://stackoverflow.com/questions/26089893/understanding-numpys-einsum .\"}\n\nRecently I've been working with some of my students on a paper for the [SciPy Conference](https://scipy2018.scipy.org/) related to our OrNet project. Part of this involved designing our own [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) (GMM). In doing so, I somewhat accidentally-on-purpose stumbled upon the beauty of `einsum`. So I wanted to briefly motivate its use (expanding upon the [short use-case I wrote on my research group's blog](https://medium.com/the-quarks/an-einsum-use-case-8dafcb933c66)) and show the performance results of re-implementing iterative computations using `einsum`.\n\n### Gaussian Madness\n\nOne of the critical steps of this algorithm is to evaluate the probability of any given point $\\vec{x}$ under the model that we've learned so far (the current values of $\\vec{\\mu}$ and $\\Sigma$. In higher dimensions (as in, higher than 1), this has a pretty tricky formula:\n\n$$\nP(\\vec{x} | \\vec{\\mu}, \\Sigma) = \\frac{\\exp{(-\\frac{1}{2}(\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1} (\\vec{x} - \\vec{\\mu}) )}} {\\sqrt{(2 \\pi)^2 \\det{(\\Sigma)} }}\n$$\n\nIt's...messy. The things that make it particularly tough to efficiently evaluate include:\n\n - the matrix multiplication in the numerator\n - the inversion of $\\Sigma$ in the numerator\n - the determinant of $\\Sigma$ in the denominator\n - **the fact that this has to be computed for *every* $\\vec{x}$ (thousands), under *every* combination of $\\mu$ and $\\Sigma$ (often a few dozen), at *every* iteration of the algorithm (often a few dozen)**\n\nBut wait! That's not all!\n\nAlso critical to the GMM are the *update* steps: re-estimating the current values of the parameters so they (hopefully) better fit your data. This happens *a lot*!\n\nIn multidimensional wonderlands, the updates of the parameters look something like this:\n\n$$\n\\vec{\\mu}_{ki} = \\frac{1}{N_k} \\sum_{n = 1}^N r_{kn} \\vec{x}_n\n$$\n\n$$\n\\Sigma_{ki} = \\frac{1}{N_k} \\sum_{n = 1}^N r_{kn} (\\vec{x}_n - \\vec{\\mu}_{ki})(\\vec{x}_n - \\vec{\\mu}_{ki})^T\n$$\n\nThe first equation for $\\vec{\\mu}_{ki}$--the $k^{th}$ component (one of the 50-100) at the $i^{th}$ iteration (often at least a few dozen)--is a \"fairly\" easy multiplication of each data point $\\vec{x}_n$ by some number $r_{kn}$, all summed up and scaled by $\\frac{1}{N_k}$.\n\nThe second equation is a lot harder.\n\nFor every $\\vec{\\mu}_{ki}$ we have a corresponding $\\Sigma_{ki}$. Which, remember, is a full matrix. This involves taking the $\\vec{\\mu}_{ki}$ we computed in the first equation, and computing outer products with every data point $\\vec{x}_n$.\n\n\"Outer product\" is a fancy name for \"matrix multiplication\"!\n\n### Bring on the Code\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy.linalg as sla\n```\n:::\n\n\nSo of course, I initially implemented these two core operations in the way that made the most sense: using `numpy` functions and Python comprehensions.\n\n**For computing $P(\\vec{x})$**: \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef px1(X, mu, sigma):\n    det = sla.det(sigma)\n    inv = sla.inv(sigma)\n    p = np.array([(x - mu).dot(inv).dot(x - mu) for x in X])\n    n = 1 / ((((2 * np.pi) ** 2) * det) ** 0.5)\n    px = np.exp(-0.5 * p) * n\n    return px\n```\n:::\n\n\nAnd **for re-computing $\\mu$ and $\\Sigma$**:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef musig1(X, r):\n    N = r.sum()\n    mu_next = (r[:, np.newaxis] * X).sum(axis = 0) / N\n    \n    s = np.zeros(shape = (X.shape[1], X.shape[1]))\n    for x, ri in zip(X - mu_next, r):\n        s += (np.outer(x, x) * ri)\n    sigma_next = s / N\n\n    return mu_next, sigma_next\n```\n:::\n\n\nThese were computational *bears*. In `px1`, the line starting `p = ...` is a *huge* computational cost: doing two dot products inside a loop over all the data points $\\vec{x}_n$ is really, really expensive.\n\nAnd in `musig1`, the operation to compute the next value of $\\Sigma$ involves computing the outer product of every $\\vec{x}_n$ subtracted by the current value of $\\vec{\\mu}$.\n\nSuffice to say, when it came time to actually run the algorithm, it shouldn't come as a surprise that, with a dataset of roughly 130,000 points in 2 dimensions, each iteration of our algorithm took about **52 seconds**.\n\n...the *vast majority* of which was spent computing dot products!\n\n![](before.png){fig-alt=\"Python profiling results before any optimizations.\"}\n\nMore than half of a single iteration was devoted to dot products in the `px1` function above. That's *bad*.\n\nJust for grins, let's time everything:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX = np.random.random(size = (130000, 2))  # To simulate the data I was working with.\nmu = X.mean(axis = 0)  # Compute an actual mean.\nsigma = np.array([[5, 0], [0, 5]])\nr = np.random.random(130000)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n%timeit px1(X, mu, sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n267 ms ± 3.88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n%timeit musig1(X, r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n563 ms ± 9.38 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\nThat's a full second of computation just for 1 of the Gaussian components (i.e., one pair of $\\mu$ and $\\Sigma$). Crank that number of components up to 50 and, well, there's your ~50-second runtime.\n\nI was admittedly at a bit of a loss here, considering I was already using NumPy constructs. But at the same time, these were highly structured linear algebra operations; there had to be something more efficient?\n\n### `einsum` to the Rescue\n\nDISCLAIMER: I should highlight that I *didn't immediately land on this*. It took some Googling around and some sitting and thinking before I figured out how this could be used. Then it took some more trial-and-error to actually get it to work.\n\nBut work, it did. Holy crap.\n\n`np.einsum` is an implementation of the [Einstein Summation](http://mathworld.wolfram.com/EinsteinSummation.html), which is a super-fancy term for a cool (but admittedly tricky-to-grasp) shorthand notation for dealing with multiplications and summations of vectors, matrices, and tensors.\n\nI won't rehash the basic examples I've found across the web that helped me understand how this works, but I will give special emphasis to two articles:\n\n 1. [Einstein Summation in NumPy](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/), and\n 2. [A basic introduction to NumPy's einsum](http://ajcr.net/Basic-guide-to-einsum/)\n \nI found #2 particularly accessible, but I'd recommend both.\n\nIt took some doing, but I managed to rewrite both equations using `einsum`s:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef px2(X, mu, sigma):\n    det = sla.det(sigma)\n    inv = sla.inv(sigma)\n\n    ###\n    p = np.einsum('ni,ji,ni->n', X - mu, inv, X - mu)\n    ###\n\n    n = 1 / ((((2 * np.pi) ** 2) * det) ** 0.5)\n    px = np.exp(-0.5 * p) * n\n    return px\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef musig2(X, r):\n    N = r.sum()\n    mu_next = (r[:, np.newaxis] * X).sum(axis = 0) / N\n\n    ###\n    sigma_next = np.einsum(\"ni,nj->ij\", (X - mu_next) * r[:, np.newaxis], X - mu_next) / N\n    ###\n\n    return mu_next, sigma_next\n```\n:::\n\n\nThe `musig2` use actually ended up being a straightforward application of the \"each row of $A$ multiplied by $B$\" rule at the end of the #2 link above, with the slight wrinkle of making the *first* axis, `n`, the one that is summed over in the result.\n\nThe one in `px2` was a lot harder, and for a time I wasn't convinced it could be done. At one point I was able to basically split the operation in two `einsums` that were more or less identical to the one in `musig2`; it basically involved a repeated application of \"multiply each row of $A$ by $B$\". But I kept hunting around the web, particularly for examples of multiplying many matrices together, and eventually managed to figure this one out.\n\nIt boiled down to basics of how `einsum` works: I knew the output had to be a list of numbers `n` long, and that the first and third inputs were identical (`ni`). The key was: if everything had 2 axes going in, but only 1 axis was coming out, then all three inputs needed to share one axis that could be summed over.\n\n### With great power blah blah blah\n\nJust for fun, let's run our examples again:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n%timeit px2(X, mu, sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6.4 ms ± 93.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n%timeit musig2(X, r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6.15 ms ± 49.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n:::\n:::\n\n\nTo sum up:\n\n - Going from a list comprehension of calls to `numpy.dot` to a single `einsum` decreased runtime of a single iteration from **300ms** to **7ms**, a speed-up of over 40x.\n - Going from a loop of `numpy.outer` products to a single `einsum` decreased runtime of a single iteration from **847ms** to **7.5ms**, a speed-up of over 110x.\n\nI re-ran `cProfile` to see what my performance gains were, and holy *crap* Batman.\n\n![](after.png){fig-alt=\"Python profiling results after using einsum.\"}\n\nNot only was my total runtime down from ~52 seconds per iteration to a mere **1.5 seconds per iteration**, but the most time-consuming block of my code was now `einsum` itself!\n\nOf course, I wasn't going to argue with a full *order of magnitude* reduction in runtime.\n\nOur SciPy paper is still under review, so feel free to [head over there and check out](https://github.com/scipy-conference/scipy_proceedings/pull/396) how we're putting GMMs to use for bioimaging!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}